Chapter 1 : Introduction
----------------------

What is Pig ? 
Pig provides an engine for executing data flows in parallel on Hadoop.
Pig is a data flow language.
It includes a language, Pig Latin, for expressing these data flows.

Pig runs on Hadoop. 
It makes use of both the Hadoop Distributed File System, HDFS, and Hadoopâ€™s processing system, MapReduce.

Word Count in pig :
lines = load '/home/devo/c4493001/pratice/processes.txt' as (line);
lines is the relation 
words = foreach lines GENERATE FLATTEN(TOKENIZE (line)) as word ;
grouped = GROUP words by word ;
wordcount = foreach grouped generate group,COUNT(words) ; 
DUMP wordcount ; 

line : 1017 table_name Mon Apr  3 07:07:24 GMT 2017 
TOKENIZE operator : first splits each line into words . It creates a bag of words 
e.g ({(1017),(table_name),(Mon),(Apr),(3),(07:07:24),(GMT),(2017)})

FLATTEN function : the bag is converted into a tuple
e.g (1017)
(table_name)
(Mon)
(Apr) and so on .... 

then the  words are grouped together so that the count can be computed 
e.g DUMP grouped gives : (Apr,{(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr),(Apr)}) 
final result : (Apr,15) 



How Pig came to Light ? 
1. Initially map reduce was the only way to code in hadoop in order to process Bigdata. 
 Then it was found there are many people around who are not proficient with java and were hence deprived of making use of Hadoop 
 to process Bigdata as they were unable to write map-reduce code, so for them scripting language like Pig was invented.

2. Pig in turn at the back converts the scripts to Map-reduce code to process the data.
3. It makes the life of developers easy as a two line code of Pig may be drill down to a page long code of pure java map reduce code.

How Pig differs from MapReduce ?
Pig provides users with several advantages over using MapReduce directly.
1. Pig Latin provides all of the standard data-processing operations, such as join, filter, group by, order by, union, etc.  
   but MapReduce provides the group by operation directly and order by operation indirectly through the way it implements 
   the grouping. Join, are not provided and must instead be written by the user.Filter and projection can be implemented 
    trivially in the map phase.
2. Pig provides some complex implementations of these standard data operations. 
   For e.g sometimes the data sent to the reducers is often skewed .Pig has join and order by operators that will handle 
   this case and (in some cases) rebalance the reducers. 
   but rewriting these in MapReduce would be time consuming. 
3. MapReduce has no opportunity to optimize or check the userâ€™s code. 
   Pig, on the other hand, can analyze a Pig Latin script and understand the data flow that the user is describing.
   i.e it can do early error checking  and optimisations 
   
All of these points mean that Pig Latin is much lower cost to write and maintain than Java code for MapReduce.      


e.g  Finding the top 3 emp salary and name 
users = Load '/home/devo/c4493001/pratice/emp_salary.txt' as (name : chararray ,salary:int) ;
grpd = foreach users generate name,salary as e_salary ;
srtd = order grpd by e_salary desc ;
final = limit srtd 3 ; 
DUMP Final ; 
store final into '/home/devo/c4493001/pratice/emp_salary' ; 
you can see emp_salary directory is created and 
bash-3.2$ cat part-r-00000
Sunil	901
Ram	837
Rohit	832

What Is Pig Useful For?
Pig Latin use cases tend to fall into three separate categories: 
a. traditional extract transform load (ETL) data pipelines 
b. research on raw data
c. iterative processing.


Chapter 2 : Running pig 
 -----------------
 You can run Pig locally on your machine or on your grid. You can also run Pig as part of Amazonâ€™s Elastic MapReduce service.
 
 Running Pig locally on your machine is referred to as local mode. 
 Local mode is useful for prototyping and debugging your Pig Latin scripts.
 
 Starting with version 0.7, it uses the Hadoop class LocalJobRunner that reads from the local filesystem and 
 executes MapReduce jobs locally. 
 
 e.g Find average Salary of all employees 
 users = Load '/home/devo/c4493001/pratice/emp_salary.txt' as (name : chararray ,salary:int) 
 grpd = GROUP users all;
 # it puts all the records in one bag as we want to sum all the emp salary 
 t_salary = foreach grpd generate SUM(users.salary) as total_salary ;
 t_count = foreach grpd generate COUNT(users.salary) as total_count ;
 t_avg =  foreach grpd generate  (double) (t_salary.total_salary/t_count.total_count)  ; 
 store t_avg into '/home/devo/c4493001/sales_aggr/emp_salary' ; 
 
 
 OR 
 t_avg =  foreach grpd generate AVG(users.salary) ; 
 o/p :
 (469.42857142857144)

Command-Line and Configuration Options
-e or -execute
Execute a single command in Pig. For example, pig -e fs -ls will list your home directory.
-h or -help
List the available command-line options.
 
 -h properties
List the properties that Pig will use if they are set by the user.
-P or -propertyFile
Specify a property file that Pig should read.
-version
Print the version of Pig.

while running in cluster you can use pig -f my_script.pig but for local execution you should use pig -x local my_script.pig 

pig -x local / pig -x mapreduce :  -x stands for execute, but if we use the -f we can pass in a file and run the Pig Script
	
Pig basically has two execution modes

1] Local Mode

2] Map-Reduce Mode

Local Mode - When you run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and local file system.

Here *all files* means all the files which you are going to process and all the jars or anything which you are referring/using in pig Script.

Mapreduce Mode - When you run Pig in mapreduce mode, you are dealing with Hadoop cluster and HDFS(Hadoop Distributed File System).

In this case *all files* are expected to be in the HDFS. 

Chapter 4 : Pig's Data Model:
--------------------------
Types
Pigâ€™s data types can be divided into two categories: 
scalar types, which contain a single value, and 
complex types, which contain other types.

 scalar types:
 ------
 int : four-byte signed intege
 long :  eight-byte signed integer e.g 5000000000L
 float : use four bytes   in some calculations it will lose precision ,use an int or long instead
 		 e.g 6.022e23f 3.14f 
 double : A double-precision floating-point number. use eight bytes to store their value ,2.71828 exponent format, 6.626e-34.
 chararray : A string or character array.e.g 'fred' 
 			 Unicode characters can be expressed as \u followed by their four-digit hexadecimal Unicode value.e.g Ctrl-A is expressed as \u0001.
 
 Complex Types
 --------
 
Pig has three complex data types: maps, tuples, and bags. 

Map : 
A map in Pig is a chararray to data element mapping, where that element can be any Pig type, 
including a complex type. The chararray is called a key and is used as an index to find the element, referred to as the value.			 	 

Tuple :
A tuple is a fixed-length, ordered collection of Pig data elements. 
Tuples are divided into fields, with each field containing one data element. 
A tuple is analogous to a row in SQL, with the fields being SQL columns.

Bag:
A bag is an unordered collection of tuples. Because it has no order, it is not possible to reference tuples in a bag by position.
It is not required to, have a schema associated with bag .
In the case of a bag, the schema describes all tuples within the bag. 

Id, product_name
-----------------------
10, iphone
20, samsung
30, Nokia


Field: A field is a piece of data. In the above data set product_name is a field. 
Tuple: A tuple is a set of fields. Here Id and product_name form a tuple. Tuples are represented by braces. Example: (10, iphone). 
Bag: A bag is collection of tuples. Bag is represented by flower braces. Example: {(10,iphone),(20, samsung),(30,Nokia)}. 
Relation: Relation represents the complete database. A relation is a bag. 
To be precise relation is an outer bag. We can call a relation as a bag of tuples.

To compare with RDBMS, a relation is a table, where as the tuples in the bag corresponds to the rows in the table. 
Note that tuples in pig doesn't require to contain same number of fields and fields in the same position have the same data type. 

Few More exercise : 

movies = load '/home/devo/c4493001/pratice/movies_data.csv' USING PigStorage(',') as (id,name,year,rating,duration) ; 
# note be careful about the case senistivity PIGSTORAGE and PigStorage are different ,so it wont work . 

sample_data = SAMPLE movies 0.01 ;
# to see the sample records 

1. List the movies that having a rating greater than 4
grpd = foreach movies generate name,rating ;
rating_greater4 = FILTER grpd BY (float)rating > 4.0 ;
DUMP rating_greater4 ;

2. List the movies that were released between 1950 and 1960
   movies_5060 = FILTER movies BY year>1950 and year<1960  
 
3. List the movies that start with the Alpahbet A
   moviesWithA = FILTER movies BY name matches 'A.*' ; 
   
To view the step-by-step execution of a sequence of statements you can use the ILLUSTRATE command:
ILLUSTRATE movies_5060 ;    

4. List all the movies in the ascending order of year. 
    asc_movies_year = ORDER movies by year DESC ;
    
5.  list the distinct records present movies_with_dups:
	no_dups = DISTINCT movies_with_dups;   

How Strongly Typed Is Pig?
In a strongly typed computer language (e.g., Java), the user must declare up front the type for all variables. 
In weakly typed languages (e.g., Perl), variables can take on values of different type and adapt as the occasion demands.
So which is Pig? For the most part it is strongly typed. 
If you describe the schema of your data, Pig expects your data to be what you said. 
But when Pig does not know the schema, it will adapt to the actual types at runtime.  

e.g 
player = load 'baseball' as (name:chararray, team:chararray,
pos:bag{t:(p:chararray)}, bat:map[]);
unintended = foreach player generate bat#'base_on_balls' - bat#'ibbs';
If Pig were weakly typed, the output of unintended would be records with one field typed as an integer. As it is, Pig will output records with one field typed as a double. 
Pig will make a guess and then do its best to massage the data into the types it guessed.	

chapter 5: Intro to Pig Latin :
-------------------------
Positional references are preceded by a $ (dollar sign) and start from 0:

prices = load '/home/devo/c4493001/pratice/NYSE_daily.txt' as  (exchange, symbol, date, open, high, low, close, volume, adj_close);
gain = foreach prices generate close-open ;
gain2 = foreach prices generate $6-$3 ;
beginning = foreach prices generate ..open; -- produces exchange, symbol, date, open
middle = foreach prices generate open..close; -- produces open, high, low, close
end = foreach prices generate volume..; -- produces volume, adj_close
	 
Join.pig 
daily = load '/home/devo/c4493001/pratice/NYSE_daily.txt' as  (exchange, symbol, date, open, high, low, close, volume, adj_close) ;
divs = load '/home/devo/c4493001/pratice/NYSE_dividends.txt' as (exchange, symbol, date, dividends) ;

jnd = join daily by (symbol,date) ,divs by (symbol,date) ;
jnd_left = join daily by (symbol,date) left outer,divs by (symbol,date) ;

Self joins are supported, though the data must be loaded twice:
divs1 = load 'NYSE_dividends' as (exchange:chararray, symbol:chararray, date:chararray, dividends);
divs2 = load 'NYSE_dividends' as (exchange:chararray, symbol:chararray, date:chararray, dividends);
	
jnd = join divs1 by symbol, divs2 by symbol ;
increased = filter jnd by divs1::date < divs2::date and divs1::dividends < divs2::dividends ;	

Problem 2: 
CSV are present in : /home/devo/c4493001/pratice 
the problem is taken from link : https://megatome.com/simple-data-analysis-with-pig-ee4390c93085 

Type head BX-Books.csv to see the first few lines of the raw data. Youâ€™ll notice thatâ€™s itâ€™s not really comma-delimited; the delimiter is â€˜;â€˜. 
There are also some escaped HTML entities we can clean up, and the quotes around all of the values can be removed.

The first line in the file looks like this:
"ISBN";"Book-Title";"Book-Author";"Year-Of-Publication";"Publisher";"Image-URL-S";"Image-URL-M";"Image-URL-L"
This lines defines the data format of the fields in the file. 

sed 's/\&amp;/\&/g' BX-Books.csv | sed -e '1d' |sed 's/;/$$$/g' | sed 's/"$$$"/";"/g' | sed 's/"//g' > BX-BooksCorrected.txt 

Above Command will:
    Replace all &amp; instances with &
    Remove the first (header) line
    Change all semicolons to $$$
    Change all "$$$" instances into ";"
    Remove all " characters
Steps 3 and 4 may look strange, but some of the field content may contain semicolons. In this case, they will be converted to $$$, but they will not match the "$$$" pattern, 
and will not be converted back into semicolons and mess up the import process.

Load the data into a Pig collection:
books = LOAD '/home/devo/c4493001/pratice/BX-BooksCorrected.txt' USING PigStorage(';') AS (ISBN:chararray,BookTitle:chararray,BookAuthor:chararray,YearofPubliccation:int,Publisher:chararray) ;
grunt> describe books ;
books: {ISBN: chararray,BookTitle: chararray,BookAuthor: chararray,YearofPubliccation: int,Publisher: chararray}

Group the collection by year of publication:
grpd = group books BY YearofPubliccation ;  	

Generate book count by year:
countByYear = foreach grpd generate group as YearofPubliccation,COUNT($1) AS BookCount ;

cleanup: 
books_refine = FILTER books BY YearofPubliccation > 0;
grpd = group books_refine BY YearofPubliccation ;  	
countByYear = foreach grpd generate group as YearofPubliccation,COUNT($1) AS BookCount ;

--to check the above count for a year 
lis = FILTER books_refine BY YearofPubliccation == 1949 ; 
dump lis ;

create a set of all authors, and all years they wrote books:
pivot = foreach (GROUP books_refine BY BookAuthor) generate group as  BookAuthor, FLATTEN(books_refine.YearofPubliccation) as Year ; 
this will create a structure like (ram singh ,1928) (ram singh ,1983) 
Create author book count by year:
grpByAuthYear = GROUP pivot BY (BookAuthor, Year);  

with_count = FOREACH grpByAuthYear GENERATE FLATTEN(group), COUNT(pivot) as count; 

author_result = FOREACH (GROUP with_count BY BookAuthor) { order_by_count = ORDER with_count BY count DESC; 
GENERATE group AS Author, order_by_count.(Year, count) AS Books;  }; 

pub_auth = FOREACH books GENERATE Publisher, BookAuthor; 
distinct_authors = FOREACH (GROUP pub_auth BY Publisher) { da = DISTINCT pub_auth.BookAuthor; 
GENERATE group AS Publisher, da AS Author; }; 

distinct_flat = FOREACH distinct_authors GENERATE Publisher, FLATTEN(Author) AS Author ;

joined = JOIN distinct_flat BY Author, author_result BY Author; 

filtered = FOREACH joined GENERATE distinct_flat::Publisher AS Publisher, distinct_flat::Author AS Author, author_result::Books AS Books ;

Generate Final Results: 
result = FOREACH (GROUP filtered BY Publisher) { order_by_pub = ORDER filtered BY Publisher ASC; 
 GENERATE group AS Publisher, order_by_pub.(Author, Books); };

create set of all authors who wrote a book in a particular year 
grpByYearAuth =  foreach grpd generate $0,books_refine.BookAuthor ;
  
  
  Remove the delimiters in Hadoop using pig : 
------------------------------------------------
we need to remove the single quotes  and extra ( in file 

(10, 'ACCOUNTING', 'NEW YORK')
(20, 'RESEARCH', 'DALLAS')
(30, 'SALES', 'CHICAGO')
(40, 'OPERATIONS', 'BOSTON')  
  
A =Load '/home/devo/c4493001/pratice/dept.txt' as (line:chararray); 
DUMP A ;
((10, 'ACCOUNTING', 'NEW YORK'))
((20, 'RESEARCH', 'DALLAS'))
((30, 'SALES', 'CHICAGO'))
((40, 'OPERATIONS', 'BOSTON'))
 
B = FOREACH A GENERATE REPLACE(line,'[\\\'\\(\\)]+','');
DUMP B ;
