Question 1
If the features of Model 1 are a strict subset of those in Model 2, the TRAINING error of the two models can never be the same.
True. 
False. c 

2. If the features of Model 1 are a strict subset of those in Model 2, which model will USUALLY have lowest TRAINING error?


Model 1.  
Model 2  c
It's impossible to tell with only this information 

dbt - does complexity of model increases with no. of features

3.If the features of Model 1 are a strict subset of those in Model 2. which model will USUALLY have lowest TEST error?
Model 1
Model 2
It's impossible to tell with only this information c 

4.If the features of Model 1 are a strict subset of those in Model 2, which model will USUALLY have lower BIAS?
Model 1 
Model 2 c
It's impossible to tell with only this information. 

7. Question 7
It is always optimal to add more features to a regression model.


True
False c 

8.A simple model with few parameters is most likely to suffer from:


High Bias c
High Variance

9.A complex model with many parameters is most likely to suffer from:


High Bias
High Variance c

10.A model with many parameters that fits training data very well but does poorly on test data is considered to be


accurate
biased
overfitted. c
poorly estimated

11. A common process for selecting a parameter like the optimal polynomial degree is:


Bootstrapping
Model estimation
Multiple regression
Minimizing test error
Minimizing validation error. c

12.  
Selecting model complexity on test data (choose all that apply):
Allows you to avoid issues of overfitting to training data 
Provides an overly optimistic assessment of performance of the resulting model  c 
Is computationally inefficient.  
Should never be done. c 


13.Which of the following statements is true (select all that apply): For a fixed model complexity, in the limit of an infinite amount of training data,  (refer slide 60)


The noise goes to 0. 
Bias goes to 0.  
Variance goes to 0. c
Training error goes to 0.   
Generalization error goes to 0. 

(refer slide 60) Variance goes to 0 -i.e spread goes 0


