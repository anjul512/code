Project -1 
link : http://www.bigsynapse.com/sampling-large-datasets-using-spark
DataSet : http://stat-computing.org/dataexpo/2009/the-data.html 

Problem Statement?

Our goal is to generate a sampled dataset. The fraction of data to be sampled will be provided as an input. I have used fraction size of 0.001 which is 0.01% of the original dataset which should translate to approximately 120000 records which is adequate for most testing of the applications we will develop on top of the airline data.

Step -1 
-----------
hadoop fs -copyFromLocal '/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/BuildingSparkApplication(orielly)/airline_2007.csv' '/user/hive/warehouse/airlinedata' 

hadoop fs -copyFromLocal '/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/BuildingSparkApplication(orielly)/airline_2008.csv' '/user/hive/warehouse/airlinedata/' 


**************************************************
create a zero byte file in hdfs
hadoop fs -touchz /user/hive/warehouse/sample1/airlinedata.txt 
17/07/10 20:03:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
**************************************************

Step -2
-----------
Now merge the 2 files to a LocalFile system 

**************************************************
hadoop fs -getmerge -nl <source file path> <local system destination path>

<src files> is the HDFS path to the directory that contains the files to be concatenated
<dist file> is the local filename of the merged file
[-nl] is an optional parameter that adds a new line in the result file.

**************************************************


hadoop fs -getmerge /user/hive/warehouse/airlinedata/* '/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/BuildingSparkApplication(orielly)/airlinedata_merge.csv'

Step-3
-------------
Now bring back the 2 files from local FS to hdfs 

hadoop fs -copyFromLocal '/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/BuildingSparkApplication(orielly)/airlinedata_merge.csv' '/user/hive/warehouse/airlinedata/' 

Step-4
--------------

First read the file into a  instance.
JAVA RDD 
JavaRDD<String> rdd = sc.textFile(inputFile);

pyspark
new_rdd=sc.textFile("hdfs://localhost:54310/user/hive/warehouse/airlinedata/airlinedata_merge.csv");


Step-5
---------------

Next sample the records and generate key-value pairs

JAVA
float fraction =  0.001f ;
rdd.sample(false, fraction)  
        .mapToPair(l->{
        String[] parts = StringUtils.splitPreserveAllTokens(l, ",");
        String yrMoDd =parts[0] + "," + parts[1] +","+ parts[2];
        return new Tuple2<String,String>(yrMoDd,l);
   });

pyspark 
fraction =  0.001

def mapToPair(line):
	parts=line
	yrMoDd=line.split(',')[0]+ ","+ line.split(',')[1]+ ","+ line.split(',')[2]
	return  (yrMoDd,parts)

header = new_rdd.first()

no_header= new_rdd.filter(lambda row:row not in header)
sampled_rdd=no_header.sample(False, fraction) 
mapped_rdd=sampled_rdd.map(mapToPair)

>>> no_header.count()
[Stage 49:=====>	(1 + 4) / 11]
14462943                                                                        
>>> sampled_rdd.count()
14384                                                                           

>>> mapped_rdd.count()
[Stage 48:>		(0 + 4) / 11]
14384                                                                           


*********************************************************
rdd = sc.parallelize(range(0, 10))
>>> rdd.sample(False, .5).collect()
[1, 2, 3, 4, 5, 6, 7, 8]


takeSample(self, withReplacement, num, seed=None) ,seed means the same number will always be choosen 
Return a fixed-size sampled subset of this RDD (currently requires numpy).

rdd = sc.parallelize(range(0, 10))
>>> rdd.takeSample(False, 5, 2)
[4, 1, 6, 2, 3]
>>> rdd.takeSample(False, 5, 2)
[4, 1, 6, 2, 3]

now lets change the seed to 3 : Now some new elements will appear as in below 1,5,7... came 
>>> rdd.takeSample(False, 5, 3)
[1, 5, 7, 6, 0]
>>> rdd.takeSample(False, 5, 3)
[1, 5, 7, 6, 0]

if num you specify more than the elements in RDD then it will have all the elements ,here all 10 elements have come 
>>> rdd.takeSample(False, 15, 3)
[1, 5, 7, 6, 0, 3, 8, 9, 4, 2]
*********************************************************
Explanation

The mapToPair method call generates tuples which have the following format

Key = {YEAR},{MONTH},{DATE}
Value = {Original Record}
Note the order of processing. We sample before we extract the key-value pairs. That way we only produce key-value pairs only on the sampled records.

How is sampling without replacement done? Lets say we need 0.01% records. Thus we need 1 out of every 1000 records. One way to get a 0.01% of the records is take 1 record for every 1000 records encountered. One of the assumptions here is that the records with the expected characteristics (Representative smaller sample, in our case) are uniformly distributed.


*********************************************************

Step-6
---------------
*********************************************************


Then we have to repartition the records. The original dataset had each year's flight records in its own file. Thus we have to re-partition this output based on year. This is where Spark truly shines. Since the original dataset was already partitioned by year, Spark will not do a full scale shuffling, it will allocate a node to the each years data (ex. 2007 data will get its own node, 2008 data will also get its own node). The data will be shuffled to this node based on the year. But if the node already has the data for the year it will not be reshuffled. Thus if 2007 data was entirely processed in one node, there will be no shuffling needed.

Finally the data will need to be sorted by Date (YYYY-MM-DD) of the flight. Sorting is similar to Map-Reduce Sort-Shuffle phase. It partially occurs in the Mapper node and partially in the Reducer node.
*********************************************************

JAVA
java code : https://github.com/sameeraxiomine/sparkusingjava8/blob/master/src/main/java/com/axiomine/spark/examples/airline/datagen/AirlineDataSampler.java

.repartitionAndSortWithinPartitions(
   new CustomPartitioner(noOfPartitions),//Partition output as the original input
   new CustomComparator()) //Sort in ascending order by Year,Month and Day
int noOfPartitions = files.size();

Repartition- changing the no. of pieces your RDD is broken into,moves data around the cluster 
 Operates in key value pairs 


pyspark
----
noOfPartitions is the file size

import os
import subprocess

cmd = ['hadoop', 'fs', '-du', '/user/hive/warehouse/airlinedata/airlinedata_merge.csv']
file_size = subprocess.check_output(cmd).strip().split(' ')[0]

# file_size - is in bytes 
file_size_GB=int(file_size)/(1024*1024*1024)

def CustomPartitioner(line):
	y2=line # converts tuple to a string 
	y1=y2[0] # extract the key 
	x=int(y1.split(',')[0]) #'2007,1,1' - key
	return  (x)

# mapped_rdd is a list of tuples
# partition_rdd=mapped_rdd.repartitionAndSortWithinPartitions(CustomPartitioner,partitionFunc=lambda x:x==2007)

partition_rdd=mapped_rdd.repartitionAndSortWithinPartitions(file_size_GB,partitionFunc=lambda x:x==CustomPartitioner)

 
*********************************************************
pairs=sc.parallelize([[1,1],[1,2],[2,3],[3,3] ]) 

pairs.repartitionAndSortWithinPartitions(2,partitionFunc=lambda x:x==1).glom().collect()
# tells how to assign items to partiions , here we say either of key ==1 
output 
[[(2, 3), (3, 3)], [(1, 1), (1, 2)]]

*********************************************************





