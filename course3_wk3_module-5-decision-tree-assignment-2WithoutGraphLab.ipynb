{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: In this assignment, we will focus on building decision trees where the data contain only binary (0 or 1) \n",
    "    features. This allows us to avoid dealing with:\n",
    "\n",
    "Multiple intermediate nodes in a split\n",
    "\n",
    "The thresholding issues of real-valued features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the lending club dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Like the previous assignment, reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan. You should have code analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x:+1 if  x==0 else -1)\n",
    "del loans['bad_loans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>final_d</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1435</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3932</td>\n",
       "      <td>20161201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501  1296599    5000       5000         4975              36 months   \n",
       "1  1077430  1314167    2500       2500         2500              60 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade    ...     delinq_2yrs_zero  \\\n",
       "0  10.65     162.87       B     B2          ...      1.0               \n",
       "1  15.27     59.83        C     C4          ...      1.0               \n",
       "\n",
       "  pub_rec_zero collections_12_mths_zero  short_emp payment_inc_ratio  \\\n",
       "0  1.0          1.0                      0          8.1435             \n",
       "1  1.0          1.0                      1          2.3932             \n",
       "\n",
       "           final_d last_delinq_none last_record_none last_major_derog_none  \\\n",
       "0  20141201T000000  1                1                1                      \n",
       "1  20161201T000000  1                1                1                      \n",
       "\n",
       "  safe_loans  \n",
       "0  1          \n",
       "1 -1          \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous assignment where we used several features, in this assignment, we will just be using 4 categorical\n",
    "features: \n",
    "\n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment.\n",
    "\n",
    "Since we are building a binary decision tree, we will have to convert these categorical features to a binary representation in a subsequent section using 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "\n",
    "#Extract these feature columns from the dataset, and discard the rest of the feature columns.\n",
    "\n",
    "loans =loans[features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grade        term home_ownership emp_length  safe_loans\n",
       "0  B      36 months  RENT           10+ years  1         \n",
       "1  C      60 months  RENT           < 1 year  -1         \n",
       "2  C      36 months  RENT           10+ years  1         "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'grade', u'term', u'home_ownership', u'emp_length', u'safe_loans'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes to people using other tools**\n",
    "\n",
    "If you are using SFrame, proceed to the section \"Subsample dataset to make sure classes are balanced\".\n",
    "\n",
    "**If you are NOT using SFrame**, download the list of indices for the training and test sets:\n",
    "\n",
    "module-5-assignment-2-train-idx.json.zip\n",
    "\n",
    "module-5-assignment-2-test-idx.json.zip\n",
    "Then follow the following steps:\n",
    "\n",
    "Apply one-hot encoding to loans. Your tool may have a function for one-hot encoding.\n",
    "\n",
    "Load the JSON files into the lists train_idx and test_idx.\n",
    "\n",
    "Perform train/validation split using train_idx and test_idx. In Pandas, for instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to loans. Your tool may have a function for one-hot encoding.\n",
    "\n",
    "\n",
    "categorical_variables =[]\n",
    "for feat_name,feat_type in zip(loans.columns,loans.dtypes):\n",
    "    if feat_type==object: # In pandas dataframe string types shows as object \n",
    "        categorical_variables.append(feat_name)\n",
    "\n",
    "#df['list_from_dict'] = [[x['name'] for x in list_dict] for list_dict in df['list_dicts']]\n",
    "\n",
    "for feature in categorical_variables:\n",
    "    loans_one_hot = loans[feature].apply(lambda x:{x:1})\n",
    "    # the above o/p will give like :  1 {u' 60 months': 1}- so need to convert it like {' 60 months': 1}which is list of dicts\n",
    "    loans_one_hot_encoded =loans_one_hot.values.tolist() # gives list of dict \n",
    "    loans_unpacked = pd.DataFrame(loans_one_hot_encoded) # gives a dataframe \n",
    "    \n",
    "    # Change NaN's to 0's\n",
    "    for columns in loans_unpacked.columns:\n",
    "        loans_unpacked[columns]=loans_unpacked[columns].fillna(0)\n",
    "        loans[columns] = loans_unpacked[columns].values\n",
    "    del loans[feature]  # removing cols ['grade', 'sub_grade', 'home_ownership', 'purpose', 'term']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'A', u'B', u'C', u'D', u'E', u'F', u'G', u' 36 months', u' 60 months',\n",
       "       u'MORTGAGE', u'OTHER', u'OWN', u'RENT', u'1 year', u'10+ years',\n",
       "       u'2 years', u'3 years', u'4 years', u'5 years', u'6 years', u'7 years',\n",
       "       u'8 years', u'9 years', u'< 1 year', u'n/a'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans.columns.drop('safe_loans')\n",
    "# Remove the response variable safe_loans'\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Load the JSON files into the lists train_idx and test_idx.\n",
    "\n",
    "# 1st read the indexes in a json file \n",
    "train_val=pd.read_json('module-5-assignment-2-train-idx.json')\n",
    "test_val=pd.read_json('module-5-assignment-2-test-idx.json')\n",
    "\n",
    "# list out the values which is ndarray\n",
    "lst_train =train_val.values.tolist()\n",
    "lst_test = test_val.values.tolist()\n",
    "# flattening the list of list to single list \n",
    "train_idx = [item for sublist in lst_train for item in sublist]\n",
    "test_idx = [item  for sublist in lst_test  for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37224 9284\n"
     ]
    }
   ],
   "source": [
    "#Perform train/validation split using train_idx and test_idx.\n",
    "\n",
    "train_data = loans.iloc[train_idx]\n",
    "test_data = loans.iloc[test_idx]\n",
    "print len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>36 months</th>\n",
       "      <th>60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>2 years</th>\n",
       "      <th>3 years</th>\n",
       "      <th>4 years</th>\n",
       "      <th>5 years</th>\n",
       "      <th>6 years</th>\n",
       "      <th>7 years</th>\n",
       "      <th>8 years</th>\n",
       "      <th>9 years</th>\n",
       "      <th>&lt; 1 year</th>\n",
       "      <th>n/a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   safe_loans    A    B    C    D    E    F    G   36 months   60 months ...   \\\n",
       "0  1           0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0         0.0        ...    \n",
       "1 -1           0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0         1.0        ...    \n",
       "\n",
       "   2 years  3 years  4 years  5 years  6 years  7 years  8 years  9 years  \\\n",
       "0  0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0       \n",
       "1  0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0       \n",
       "\n",
       "   < 1 year  n/a  \n",
       "0  0.0       0.0  \n",
       "1  1.0       0.0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note. **Some elements in loans are included neither in train_data nor test_data. This is to perform sampling to achieve class balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now proceed to the section \"Decision tree implementation\", skipping three sections below.\n",
    "#4.Subsample dataset to make sure classes are balanced\n",
    "\n",
    "#Transform categorical data into binary features\n",
    "\n",
    "#Train-test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree implementation\n",
    "\n",
    "In this section, we will implement binary decision trees from scratch.\n",
    "\n",
    "There are several steps involved in building a decision tree. For that reason, we have split the entire assignment into several sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to count number of mistakes while predicting majority class**\n",
    "\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the majority class for all data points that belong to this node. Now, we will write a function that calculates the number of misclassified examples when predicting the majority class. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node.\n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of safe loans and risky loans.\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **mistakes**.\n",
    "* ** Step 3:** Return the number of **mistakes**.\n",
    "\n",
    "\n",
    "Now, let us write the function `intermediate_node_num_mistakes` which computes the number of misclassified examples of an intermediate node given the set of labels (y values) of the data points contained in the node. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loans['safe_loans']==-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    ## YOUR CODE HERE\n",
    "    safe =sum(labels_in_node==1)\n",
    "    \n",
    "    # Count the number of -1's (risky loans)\n",
    "    ## YOUR CODE HERE\n",
    "    risky = sum(labels_in_node==-1)\n",
    "                \n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    # All the data points that are not in the majority class are considered mistakes\n",
    "    ## YOUR CODE HERE\n",
    "    if safe<risky:\n",
    "        num_mistakes=safe\n",
    "    else :\n",
    "        num_mistakes=risky\n",
    "    return num_mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are several steps in this assignment, we have introduced some stopping points where you can check your code and make sure it is correct before proceeding. To test your `intermediate_node_num_mistakes` function, run the following code until you get a **Test passed!**, then you should proceed. Otherwise, you should spend some time figuring out where things went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test passed!\n",
      "Test passed!\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 1 failed... try again!'\n",
    "\n",
    "# Test case 2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 2 failed... try again!'\n",
    "    \n",
    "# Test case 3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 3 failed... try again!'\n",
    "\n",
    "# Test case 4\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, -1, -1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 0:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 4 failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **best_splitting_feature** takes 3 arguments: \n",
    "1. The data (SFrame of data which includes all of the feature columns and label column)\n",
    "2. The features to consider for splits (a list of strings of column names to consider for splits)\n",
    "3. The name of the target/label column (string)\n",
    "\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on.\n",
    "\n",
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Follow these steps: \n",
    "* **Step 1:** Loop over each feature in the feature list\n",
    "* **Step 2:** Within the loop, split the data into two groups: one group where all of the data has feature value 0 or False (we will call this the **left** split), and one group where all of the data has feature value 1 or True (we will call this the **right** split). Make sure the **left** split corresponds with 0 and the **right** split corresponds with 1 to ensure your implementation fits with our implementation of the tree building process.\n",
    "* **Step 3:** Calculate the number of misclassified examples in both groups of data and use the above formula to compute the **classification error**.\n",
    "* **Step 4:** If the computed error is smaller than the best error found so far, store this **feature and its error**.\n",
    "\n",
    "This may seem like a lot, but we have provided pseudocode in the comments in order to help you implement the function correctly.\n",
    "\n",
    "**Note:** Remember that since we are only dealing with binary features, we do not have to consider thresholds for real-valued features. This makes the implementation of this function much easier.\n",
    "\n",
    "Fill in the places where you find `## YOUR CODE HERE`. There are **five** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data ,features,target):\n",
    "    best_feature = None # Keep track of the best feature\n",
    "    best_error =10 # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "    \n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(data)) \n",
    "    \n",
    "    #Step 1: Loop over each feature in the feature list\n",
    "    for feat in features:\n",
    "         #Step 2: Within the loop, split the data into two groups\n",
    "        #The left split will have all data points where the feature value is 0\n",
    "        left_split= data[data[feat]==0]\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        right_split = data[data[feat]==1]\n",
    "        \n",
    "        #Step 3: Calculate the number of misclassified examples in both groups of data and use the above formula \n",
    "        #to compute the classification error.\n",
    "        left_misclassified = intermediate_node_num_mistakes(left_split[target])\n",
    "        right_misclassified = intermediate_node_num_mistakes(right_split[target])\n",
    "        error = (left_misclassified + right_misclassified) / num_data_points \n",
    "        \n",
    "        #Step 4: If the computed error is smaller than the best error found so far, store this feature and its error.\n",
    "        if (error < best_error):\n",
    "            best_error = error\n",
    "            best_feature= feat\n",
    "    \n",
    "    return best_feature # Return the best feature we found        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 36 months'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_splitting_feature(train_data, features, 'safe_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your best_splitting_feature function, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "if best_splitting_feature(train_data, features, 'safe_loans') == ' 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    # create a leaf node \n",
    "    leaf ={'splitting_feature':None,\n",
    "          'left':None,\n",
    "          'right' :None,\n",
    "          'is_leaf' : True,\n",
    "          'prediction':None}\n",
    "    \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values==1])\n",
    "    num_minus_ones = len(target_values[target_values == -1]) \n",
    "    \n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1\n",
    "    else:\n",
    "        leaf['prediction'] = -1\n",
    "        \n",
    "    # Return the leaf node  \n",
    "    return leaf\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function that learns the decision tree recursively and implements 3 stopping conditions:\n",
    "1. **Stopping condition 1:** All data points in a node are from the same class.\n",
    "2. **Stopping condition 2:** No more features to split on.\n",
    "3. **Additional stopping condition:** In addition to the above two stopping conditions covered in lecture, in this assignment we will also consider a stopping condition based on the **max_depth** of the tree. By not letting the tree grow too deep, we will save computational effort in the learning process. \n",
    "\n",
    "Now, we will write down the skeleton of the learning algorithm. Fill in the places where you find `## YOUR CODE HERE`. There are **seven** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node.\n",
    "    # Recall you wrote a function intermediate_node_num_mistakes to compute this.)\n",
    "    if  intermediate_node_num_mistakes(target_values) == 0:  ## YOUR CODE HERE\n",
    "        print \"Stopping condition 1 reached.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2 (check if there are remaining features to consider splitting on)\n",
    "    if len(remaining_features)==0:   ## YOUR CODE HERE\n",
    "        print \"Stopping condition 2 reached.\"    \n",
    "        # If there are no remaining features to consider, make current node a leaf node\n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth >= max_depth:  ## YOUR CODE HERE\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Find the best splitting feature (recall the function best_splitting_feature implemented above)\n",
    "    ## YOUR CODE HERE\n",
    "    splitting_feature = best_splitting_feature(data,remaining_features,target)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]      ## YOUR CODE HERE\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    # i.e all data have same target value\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target])\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        ## YOUR CODE HERE\n",
    "        return create_leaf(right_split[target])\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, current_depth + 1, max_depth)        \n",
    "    ## YOUR CODE HERE\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, max_depth)        \n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get 'Test passed' before proceeding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', ' 36 months', ' 60 months', 'MORTGAGE', 'OTHER', 'OWN', 'RENT', '1 year', '10+ years', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '< 1 year', 'n/a']\n"
     ]
    }
   ],
   "source": [
    "#we need to drop target from independent features:train_data.drop('safe_loans',axis=1)\n",
    "x=train_data.drop('safe_loans',axis=1)\n",
    "features_new = [col for col in x.columns]\n",
    "print features_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature  36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature B. (8074, 1048)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8074 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1048 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature n/a. (96, 5)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (96 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (5 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "Split on feature E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "Split on feature A. (4701, 0)\n",
      "Creating leaf node.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "small_data_decision_tree = decision_tree_create(train_data,features_new, \n",
    "                                                target,max_depth = 3)\n",
    "if count_nodes(small_data_decision_tree) == 13:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found                :', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there : 13' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree!\n",
    "\n",
    "Now that all the tests are passing, we will train a tree model on the **train_data**. Limit the depth to 6 (**max_depth = 6**) to make sure the algorithm doesn't run for too long. Call this tree **my_decision_tree**. \n",
    "\n",
    "**Warning**: This code block may take 1-2 minutes to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature  36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature B. (8074, 1048)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8074 data points).\n",
      "Split on feature C. (5884, 2190)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5884 data points).\n",
      "Split on feature D. (3826, 2058)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3826 data points).\n",
      "Split on feature E. (1693, 2133)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1693 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2133 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2058 data points).\n",
      "Split on feature E. (2058, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2190 data points).\n",
      "Split on feature D. (2190, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1048 data points).\n",
      "Split on feature 5 years. (969, 79)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (969 data points).\n",
      "Split on feature C. (969, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (79 data points).\n",
      "Split on feature MORTGAGE. (34, 45)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (34 data points).\n",
      "Split on feature C. (34, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (45 data points).\n",
      "Split on feature C. (45, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature n/a. (96, 5)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (96 data points).\n",
      "Split on feature < 1 year. (85, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (85 data points).\n",
      "Split on feature B. (85, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (11 data points).\n",
      "Split on feature B. (11, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (5 data points).\n",
      "Split on feature B. (5, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "Split on feature E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "Split on feature F. (21666, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (21666 data points).\n",
      "Split on feature n/a. (20734, 932)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (20734 data points).\n",
      "Split on feature G. (20638, 96)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (20638 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (96 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (932 data points).\n",
      "Split on feature A. (702, 230)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (702 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (230 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (358 data points).\n",
      "Split on feature 8 years. (347, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (347 data points).\n",
      "Split on feature A. (347, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature OWN. (9, 2)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (9 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "Split on feature A. (1276, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "Split on feature A. (4701, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "# Make sure to cap the depth at 6 by using max_depth = 6\n",
    "my_decision_tree = decision_tree_create(train_data, features_new, 'safe_loans', max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Below, we call this function `classify`, which takes in a learned `tree` and a test point `x` to classify.  We include an option `annotate` that describes the prediction path when set to `True`.\n",
    "\n",
    "Fill in the places where you find `## YOUR CODE HERE`. There is **one** place in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "               ### YOUR CODE HERE\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the first example of the test set and see what my_decision_tree model predicts for this data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' 36 months': 0.0,\n",
       " ' 60 months': 1.0,\n",
       " '1 year': 0.0,\n",
       " '10+ years': 0.0,\n",
       " '2 years': 1.0,\n",
       " '3 years': 0.0,\n",
       " '4 years': 0.0,\n",
       " '5 years': 0.0,\n",
       " '6 years': 0.0,\n",
       " '7 years': 0.0,\n",
       " '8 years': 0.0,\n",
       " '9 years': 0.0,\n",
       " '< 1 year': 0.0,\n",
       " 'A': 0.0,\n",
       " 'B': 0.0,\n",
       " 'C': 0.0,\n",
       " 'D': 1.0,\n",
       " 'E': 0.0,\n",
       " 'F': 0.0,\n",
       " 'G': 0.0,\n",
       " 'MORTGAGE': 0.0,\n",
       " 'OTHER': 0.0,\n",
       " 'OWN': 0.0,\n",
       " 'RENT': 1.0,\n",
       " 'n/a': 0.0,\n",
       " 'safe_loans': -1.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test = dict(zip(test_data.columns, test_data.iloc[0,:]))\n",
    "my_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "print 'Predicted class: %s ' % classify(my_decision_tree, test_data.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's add some annotations to our prediction to see what the prediction path was that lead to this predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on  36 months = 0.0\n",
      "Split on A = 0.0\n",
      "Split on B = 0.0\n",
      "Split on C = 0.0\n",
      "Split on D = 1.0\n",
      "At leaf, predicting -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, test_data.iloc[0,:], annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What was the feature that my_decision_tree first split on while making the prediction for test_data[0]?\n",
    "\n",
    "\n",
    "36 months is root \n",
    "\n",
    "**Quiz Question:** What was the first feature that lead to a right split of test_data[0]?\n",
    "\n",
    "grade.D\n",
    "\n",
    "**Quiz Question:** What was the last feature split on before reaching a leaf node for test_data[0]?\n",
    "\n",
    "grade.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe_loans'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data, target):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction=[]\n",
    "    for j in xrange(len(data)):\n",
    "        #print i\n",
    "        prediction.append(classify(tree, data.iloc[j]))\n",
    "\n",
    "    #prediction = data.apply(lambda x: classify(tree, x))\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    ## YOUR CODE HERE\n",
    "    num_errors = 0 \n",
    "    for i in xrange(len(data)):\n",
    "        if data[target].iloc[i]!=prediction[i]:\n",
    "            num_errors+=1\n",
    "    return num_errors/float(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3837785437311504"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(my_decision_tree, test_data, target) # target is safe_loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz Question: Rounded to 2nd decimal point, what is the classification error of my_decision_tree on the test_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump\n",
    "As discussed in the lecture, we can print out a single decision stump (printing out the entire tree is left as an exercise to the curious reader).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    #split_feature, split_value = split_name.split('.')\n",
    "    print '                       %s' % name\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]                      [{0} == 1]'.format(split_name)\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [ 36 months == 0]                      [ 36 months == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What is the feature that is used for the split at the root node?\n",
    "\n",
    "### Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "* `my_decision_tree['left']` to go left\n",
    "* `my_decision_tree['right']` to go right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  feature that is used for the split at the root  - 36 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [A == 0]                      [A == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the left subtree of the left subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [B == 0]                      [B == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What is the path of the **first 3 feature splits** considered along the **left-most** branch of **my_decision_tree**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What is the path of the **first 3 feature splits** considered along the **right-most** branch of **my_decision_tree**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [C == 0]                      [C == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [D == 0]                      [D == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left']['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quiz Question: What is the path of the first 3 feature splits considered along the left-most branch of my_decision_tree?\n",
    "\n",
    "#   36 months ->  .A-> .B "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the right subtree of the right subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       D\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [D == 0]                      [D == 1]\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['right']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right']['right'], my_decision_tree['right']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quiz Question: What is the path of the first 3 feature splits considered along the right-most branch of my_decisi\n",
    "# . 36 months ->   .D->leaf\n",
    "# . 36 months,  .D, no third feature because second split resulted in leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
