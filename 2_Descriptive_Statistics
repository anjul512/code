Descriptive Statistics
----------------------
Descriptive statistics can give you great insight into the shape of each attribute. Often you can
create more summaries than you have time to review. The describe() function on the Pandas
DataFrame lists 8 statistical properties of each attribute. They are:
 Count.
 Mean.
 Standard Deviation.
 Minimum Value.
 25th Percentile.
 50th Percentile (Median).
 75th Percentile.
 Maximum Value.


# Statistical Summary
from pandas import read_csv
from pandas import set_option
filename='C:\Users\pbishwal012918\Documents\TechStuffs\dataset\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
set_option('display.width', 100)
set_option('precision', 3)
description = data.describe()
print(description)

		  preg     plas     pres     skin     test     mass     pedi      age    class
count  768.000  768.000  768.000  768.000  768.000  768.000  768.000  768.000  768.000
mean     3.845  120.895   69.105   20.536   79.799   31.993    0.472   33.241    0.349
std      3.370   31.973   19.356   15.952  115.244    7.884    0.331   11.760    0.477
min      0.000    0.000    0.000    0.000    0.000    0.000    0.078   21.000    0.000
25%      1.000   99.000   62.000    0.000    0.000   27.300    0.244   24.000    0.000
50%      3.000  117.000   72.000   23.000   30.500   32.000    0.372   29.000    0.000
75%      6.000  140.250   80.000   32.000  127.250   36.600    0.626   41.000    1.000
max     17.000  199.000  122.000   99.000  846.000   67.100    2.420   81.000    1.000

Class Distribution (Classification Only)
----------------------------------------
On classification problems you need to know how balanced the class values are. Highly imbalanced
problems (a lot more observations for one class than another) are common and may need special
handling in the data preparation stage of your project.

from pandas import read_csv
from pandas import set_option
filename='C:\Users\pbishwal012918\Documents\TechStuffs\dataset\pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
class_counts=data.groupby('class').size()
class_counts
class
0    500
1    268
dtype: int64

>>> data.groupby('age').size()
age
21    63
22    72
23    38
24    46
....
....

Correlations Between Attributes
-------------------------------
Correlation refers to the relationship between two variables and how they may or may not
change together.
The most common method for calculating correlation is Pearson's Correlation
Coefficient, that assumes a normal distribution of the attributes involved.
A correlation of -1 or 1 shows a full negative or positive correlation respectively. 
Whereas a value of 0 shows no correlation at all. 
Some machine learning algorithms like linear and logistic regression can suffer
poor performance if there are highly correlated attributes in your dataset.

# Pairwise Pearson correlations
from pandas import read_csv
from pandas import set_option
filename = "pima-indians-diabetes.data.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
set_option('display.width', 100)
set_option('precision', 3)
correlations = data.corr(method='pearson')
print(correlations)

>>> correlations
        preg   plas   pres   skin   test   mass   pedi    age  class
preg   1.000  0.129  0.141 -0.082 -0.074  0.018 -0.034  0.544  0.222
plas   0.129  1.000  0.153  0.057  0.331  0.221  0.137  0.264  0.467
pres   0.141  0.153  1.000  0.207  0.089  0.282  0.041  0.240  0.065
skin  -0.082  0.057  0.207  1.000  0.437  0.393  0.184 -0.114  0.075
test  -0.074  0.331  0.089  0.437  1.000  0.198  0.185 -0.042  0.131
mass   0.018  0.221  0.282  0.393  0.198  1.000  0.141  0.036  0.293
pedi  -0.034  0.137  0.041  0.184  0.185  0.141  1.000  0.034  0.174
age    0.544  0.264  0.240 -0.114 -0.042  0.036  0.034  1.000  0.238
class  0.222  0.467  0.065  0.075  0.131  0.293  0.174  0.238  1.000

Skew of Univariate Distributions
--------------------------------
Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or
squashed in one direction or another. 
Many machine learning algorithms assume a Gaussian distribution.

# Skew for each attribute
from pandas import read_csv
filename = "pima-indians-diabetes.data.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
skew = data.skew()
print(skew)

>>> skew
preg     0.902
plas     0.174
pres    -1.844
skin     0.109
test     2.272
mass    -0.429
pedi     1.920
age      1.130
class    0.635
dtype: float64


Data Visualisation
------------------
Univariate Plots 
1.Histogram
2.Denisty plots
3.Box and whisker plots

Histograms
==========
Histograms group data into bins and provide you a count of the number of observations in each bin. 
From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed
or even has an exponential distribution.

# Univariate Histograms
from matplotlib import pyplot
from pandas import read_csv
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
data.hist()
pyplot.show()



Density Plots
=============
Density plots are another way of getting a quick idea of the distribution of each attribute. 
The plots look like an abstracted histogram with a smooth curve drawn through the top of each bin,
much like your eye tried to do with the histograms.

# Univariate Density Plots
from matplotlib import pyplot
from pandas import read_csv
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)
pyplot.show()

Box and Whisker Plots
=====================
Boxplots summarize the distribution of each attribute, drawing a line for the median (middle value) 
and a box around the 25th and 75th percentiles (the middle 50% of the data). 
The whiskers give an idea of the spread of the data and dots outside of the whiskers
show candidate outlier values (values that are 1.5 times greater than the size of spread of the
middle 50% of the data)

# Box and Whisker Plots
from matplotlib import pyplot
from pandas import read_csv
filename = "pima-indians-diabetes.data.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)
pyplot.show()

Multivariate Plots 
1.Correlation Matrix Plot 
2.Scatter Plot Matrix 

Correlation Matrix Plot
=======================

Correlation gives an indication of how related the changes are between two variables. 
If two variables change in the same direction they are positively correlated. 
If they change in opposite directions together (one goes up, one goes down), then they are negatively correlated. 
You can calculate the correlation between each pair of attributes. 
This is called a correlation matrix. 
You can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. 
This is useful to know, because some machine learning algorithms like linear and logistic regression 
can have poor performance if there are highly correlated input variables in your data.


# Correction Matrix Plot
from matplotlib import pyplot
from pandas import read_csv
import numpy
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
correlations = data.corr()
# plot correlation matrix
fig = pyplot.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = numpy.arange(0,9,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
pyplot.show()

The example is not generic in that it specifies the names for the attributes along the axes as
well as the number of ticks. This recipe cam be made more generic by removing these aspects
as follows:

# Correction Matrix Plot (generic)
from matplotlib import pyplot
from pandas import read_csv
import numpy
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
correlations = data.corr()
# plot correlation matrix
fig = pyplot.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
pyplot.show()


Scatter Plot Matrix
===================
A scatter plot shows the relationship between two variables as dots in two dimensions, one
axis for each attribute. 
You can create a scatter plot for each pair of attributes in your data.
Drawing all these scatter plots together is called a scatter plot matrix. 
Scatter plots are useful for spotting structured relationships between variables, like whether you could summarize the
relationship between two variables with a line. 
Attributes with structured relationships may also be correlated and good candidates for removal from your dataset.

# Scatterplot Matrix
from matplotlib import pyplot
from pandas import read_csv
from pandas.tools.plotting import scatter_matrix
filename = "pima-indians-diabetes.data.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
data = read_csv(filename, names=names)
scatter_matrix(data)
pyplot.show()


Prepare your Data for ML
------------------------

The scikit-learn library provides two standard idioms for transforming data. 
Each are useful in different circumstances. The transforms are calculated in such a way that they can be 
applied to your training data and any samples of data you may have in the future. 
The scikit-learn documentation has some information on how to use various different pre-processing methods:
 Fit and Multiple Transform.
 Combined Fit-And-Transform.

The Fit and Multiple Transform method is the preferred approach. You call the fit()
function to prepare the parameters of the transform once on your data. Then later you can use
the transform() function on the same data to prepare it for modeling and again on the test or
validation dataset or new data that you may see in the future. 

The Combined Fit-And-Transform is a convenience that you can use for one off tasks. This might be useful if you are interested
in plotting or summarizing the transformed data.

Rescale Data
============
When your data is comprised of attributes with varying scales, many machine learning algorithms
can beneift from rescaling the attributes to all have the same scale. Often this is referred to as
normalization and attributes are often rescaled into the range between 0 and 1.

# Rescale data (between 0 and 1)
from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import MinMaxScaler
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(X)
# summarize transformed data
set_printoptions(precision=3)
print(rescaledX[0:5,:])

>>> rescaledX[0:5,:]
array([[ 0.353,  0.744,  0.59 ,  0.354,  0.   ,  0.501,  0.234,  0.483],
       [ 0.059,  0.427,  0.541,  0.293,  0.   ,  0.396,  0.117,  0.167],
       [ 0.471,  0.92 ,  0.525,  0.   ,  0.   ,  0.347,  0.254,  0.183],
       [ 0.059,  0.447,  0.541,  0.232,  0.111,  0.419,  0.038,  0.   ],
       [ 0.   ,  0.688,  0.328,  0.354,  0.199,  0.642,  0.944,  0.2  ]])

Standardize Data
=================

Standardization is a useful technique to transform attributes with a Gaussian distribution and
differing means and standard deviations to a standard Gaussian distribution with a mean of
0 and a standard deviation of 1.

# Standardize data (0 mean, 1 stdev)
from sklearn.preprocessing import StandardScaler
from pandas import read_csv
from numpy import set_printoptions
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
scaler = StandardScaler().fit(X)
rescaledX = scaler.transform(X)
# summarize transformed data
set_printoptions(precision=3)
print(rescaledX[0:5,:])

[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]
 [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]
 [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]
 [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]
 [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]

Normalize Data
==============
 
Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called
a unit norm or a vector with the length of 1 in linear algebra). 
This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying
scales when using algorithms that weight input values such as neural networks and algorithms that
use distance measures such as k-Nearest Neighbors.

# Normalize data (length of 1)
from sklearn.preprocessing import Normalizer
from pandas import read_csv
from numpy import set_printoptions
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
scaler = Normalizer().fit(X)
normalizedX = scaler.transform(X)
# summarize transformed data
set_printoptions(precision=3)
print(normalizedX[0:5,:])

The rows are normalised to length 1
[[ 0.034  0.828  0.403  0.196  0.     0.188  0.004  0.28 ]
 [ 0.008  0.716  0.556  0.244  0.     0.224  0.003  0.261]
 [ 0.04   0.924  0.323  0.     0.     0.118  0.003  0.162]
 [ 0.007  0.588  0.436  0.152  0.622  0.186  0.001  0.139]
 [ 0.     0.596  0.174  0.152  0.731  0.188  0.01   0.144]]

Binarize Data (Make Binary)
===========================

You can transform your data using a binary threshold. All values above the threshold are
marked 1 and all equal to or below are marked as 0. This is called binarizing your data or
thresholding your data. It can be useful when you have probabilities that you want to make crisp
values.

# binarization
from sklearn.preprocessing import Binarizer
from pandas import read_csv
from numpy import set_printoptions
filename = 'pima-indians-diabetes.data.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
binarizer = Binarizer(threshold=0.0).fit(X)
binaryX = binarizer.transform(X)
# summarize transformed data
set_printoptions(precision=3)
print(binaryX[0:5,:])
