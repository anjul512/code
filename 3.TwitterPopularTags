// 2. Example of Twitter Popular Tags  

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.twitter.TwitterUtils

/*
 Calculates popular hashtags over sliding 10 and 60 second windows from a Twitter stream .
 This stream is instantianted with credentials and optionally filters supplied by the command line arguments .
 */

object TwitterPopularTags extends App   {
  
 
  if (args.length < 4) {
    System.err.println("Provide all parameters")
    System.exit(1)
  }
  
  //StreamingExamples.setStreamingLogLevels()
  
  val Array(consumerKey,consumerSecret,accessToken,accessTokenSecret) = args.take(4)
  val filters = args.takeRight(args.length-4) // filter in our case is narendramodi  ie the last arg 
  
  //Set the system properties so that Twitter4j library used by twitter stream 
  System.setProperty("twitter4j.oauth.consumerKey", consumerKey)
  System.setProperty("twitter4j.oauth.consumerSecret", consumerSecret)
  System.setProperty("twitter4j.oauth.accessToken", accessToken)
  System.setProperty("twitter4j.oauth.accessTokenSecret", accessTokenSecret)
  
  // 9o6bMytdK03TezQ3H8x83UklJ 1dFuzX9lF9mbUoJYc6gwC9pjTDY0s3dG2tq9veLaSHkzIWClX3 913410683135434753-yp8Bzpil1P7vWcf1MrdllgkF1fFiwWm EBfYdCzBjnMXsVLFCIGAd4djoYNMWTFovUUnAS6ZdWxOy narendramodi
  
  val sparkConf = new SparkConf().setAppName("Twitter popular Tags").setMaster("local[2]")
  val ssc = new StreamingContext(sparkConf,Seconds(2))   
  val stream = TwitterUtils.createStream(ssc, None ,filters)
  
  val hashTags = stream.flatMap(status => status.getText.split(" ").filter(_.startsWith("#")))
  
  // Reduce last 60 seconds of data, every 2 seconds
  val topCounts60 = hashTags.map(( _, 1)).reduceByKeyAndWindow(_+_ , Seconds(60) )
                    .map{ case (topic,count) => (count,topic)}
                    .transform(_.sortByKey(false)) 
             
   val topCounts10 = hashTags.map(( _, 1)).reduceByKeyAndWindow(_+_ , Seconds(10) )
                    .map{case (topic,count) => (count,topic)}
                    .transform(_.sortByKey(false)) 
                 
   //print popular hashtags 
    
    topCounts60.foreachRDD( rdd => {
      val topList = rdd.take(10) 
      println("\n Popular topics in last 60 seconds (%s total):".format(rdd.count()))
      topList.foreach{case (count,tag) => println("%s (%s tweets)".format(tag,count))}
    } )               
                    
    topCounts10.foreachRDD( rdd => {
      val topList = rdd.take(10) 
      println("\n Popular topics in last 10 seconds (%s total):".format(rdd.count()))
      topList.foreach{case (count,tag) => println("%s (%s tweets)".format(tag,count))}
    } )        
    
  ssc.start() // Start the computation
  ssc.awaitTermination() // Wait for the computation to terminate

 
 
}

/*
<dependency>
  		<groupId>org.apache.spark</groupId>
  		<artifactId>spark-core_2.11</artifactId>
  		<version>1.5.2</version>
  	</dependency>
  	<dependency>
  		<groupId>org.apache.spark</groupId>
  		<artifactId>spark-streaming_2.11</artifactId>
  		<version>1.5.2</version>
  	</dependency>
  	<dependency>
  		<groupId>org.apache.spark</groupId>
  		<artifactId>spark-streaming-twitter_2.11</artifactId>
  		<version>1.6.2</version>
  	</dependency>
  </dependencies>
*/
