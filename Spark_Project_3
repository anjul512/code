Spark_Project_3
=================
dataset :https://github.com/dgadiraju/data

Spark different file formats and compression
===================================================


spark = SparkSession.builder.appName("BaseBall Analysis").config("spark.some.config.option", "some-value").getOrCreate()

orders =sc.textFile("/Users/pbishwal/Documents/Techie/SparknScala/Datasets/data-master/retail_db/orders/")

let us write in a particular file format 
create a dataframe out of the orders 
case class created in scala 
case class order(order_id:Int,order_dt:String,order_customer_id:Int,order_status:String)

in Pyspark function is created 
from pyspark.sql import  Row

def order_fn (line):
	tempList=[]
	for i in line.split(','):
		tempList.append(i)
	return [  tempList[0],tempList[1],tempList[2],tempList[3] ]

orders_data=orders.map(order_fn)		
orders_data2 =orders_data.map(lambda p:Row(
								order_id   =p[0]
								,order_dt    =p[1]
								,order_customer_id =p[2]
								,order_status    =p[3]
								))
now create a dataFrame 
orders_data_DF=spark.createDataFrame(orders_data2)

dataFrame writer - write to hdfs path : Note orders_parquet shdnt exist earlier 

--write to parquet
>>> orders_data_DF.write.parquet("hdfs://localhost:54310/user/retail_db/orders_parquet")
[Stage 48:>                                                         (0 + 2) / 2]SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

--write to json
orders_data_DF.write.json("hdfs://localhost:54310/user/retail_db/orders_json")

-- create a sqlcontext 
from pyspark.sql import SQLContext
from pyspark.sql import HiveContext
from pyspark.sql import Row, functions as F
from pyspark.sql.functions import  *

orders_data_DF.createOrReplaceTempView("orders_vw")

orders_by_status=spark.sql("select order_status,count(*) as order_status_cnt from orders_vw group by order_status")
# note put alias fr count(*) else error will be found while writing and saving parquet and avro 
+---------------+--------+                                                      
|   order_status|count(1)|
+---------------+--------+
|PENDING_PAYMENT|   15030|
|       COMPLETE|   22899|
|        ON_HOLD|    3798|
| PAYMENT_REVIEW|     729|
|     PROCESSING|    8275|
|         CLOSED|    7556|
|SUSPECTED_FRAUD|    1558|
|        PENDING|    7610|
|       CANCELED|    1428|
+---------------+--------+
# write is availabe in DF and read is available in sqlContext 

orders_by_status.write.json("hdfs://localhost:54310/user/retail_db/orders_by_status_json")

it has created 200 files 
Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -ls /user/retail_db/orders_by_status_json | wc -l 
     202
lets take the last file (most files may be 0 bytes so take the files having count)

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -cat /user/retail_db/orders_by_status_json/part-r-00185-ed7679a2-e850-47ec-a3f5-a8facb9e6c42.json
{"order_status":"CANCELED","count(1)":1428}

note if you write  the json as above then you may get unnecessarily 0 bytes files ~200 files 

sqlContext.setConf("spark.sql.shuffle.partitions", 2) 
#default is 1

now remove the directory created earlier :
hadoop fs -rm -r /user/retail_db/orders_by_status_json/
 Deleted /user/retail_db/orders_by_status_json

now lets write again
orders_by_status.write.json("hdfs://localhost:54310/user/retail_db/orders_by_status_json")

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -ls /user/retail_db/orders_by_status_json/ | wc -l
       4

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -cat /user/retail_db/orders_by_status_json/part-r-00000-a7d63fe8-b182-4ac0-a026-2305e7b7588c.json
{"order_status":"PENDING_PAYMENT","count(1)":15030}
{"order_status":"PROCESSING","count(1)":8275}
{"order_status":"PAYMENT_REVIEW","count(1)":729}
{"order_status":"PENDING","count(1)":7610}
{"order_status":"ON_HOLD","count(1)":3798}
{"order_status":"SUSPECTED_FRAUD","count(1)":1558}

-now read the json file 

sqlContext.read.json("hdfs://localhost:54310/user/retail_db/orders_by_status_json").show()
+--------+---------------+
|count(1)|   order_status|
+--------+---------------+
|   15030|PENDING_PAYMENT|
|    8275|     PROCESSING|
|     729| PAYMENT_REVIEW|
|    7610|        PENDING|
|    3798|        ON_HOLD|
|    1558|SUSPECTED_FRAUD|
|    7556|         CLOSED|
|   22899|       COMPLETE|
|    1428|       CANCELED|
+--------+---------------+

===========================
Parquet 
orders_by_status.write.parquet("hdfs://localhost:54310/user/retail_db/orders_by_status_parquet")

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -ls /user/retail_db/orders_by_status_parquet | wc -l 
       4

you can't read directly so use sqlContext to read        
sqlContext.read.parquet("hdfs://localhost:54310/user/retail_db/orders_by_status_parquet").show()
+---------------+----------------+
|   order_status|order_status_cnt|
+---------------+----------------+
|PENDING_PAYMENT|           15030|
|     PROCESSING|            8275|
| PAYMENT_REVIEW|             729|
|        PENDING|            7610|
|        ON_HOLD|            3798|
|SUSPECTED_FRAUD|            1558|
|         CLOSED|            7556|
|       COMPLETE|           22899|
|       CANCELED|            1428|
+---------------+----------------+


==================
For avro file format you need to register the dependencies 
cd /Users/pbishwal/spark-2.0.2-bin-hadoop2.7/bin

For scala 
./spark-shell --packages com.databricks:spark-avro_2.11:2.0.2 --master 	yarn --conf spark.ui.port=19635
Exception in thread "main" java.lang.Exception: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.

pyspark 

orders_by_status.write.format("com.databricks.spark.avro").save("hdfs://localhost:54310/user/retail_db/orders_by_status_avro")

above command gives error: so cd /Users/pbishwal/spark-2.0.2-bin-hadoop2.7/bin 
install packages 
./spark-shell --packages com.databricks:spark-avro_2.11:3.2.0  => launch scala shell 
./pyspark --packages com.databricks:spark-avro_2.11:3.2.0.   => launch pyspark shell 

orders_by_status=spark.sql("select order_status,count(*) as order_status_cnt from orders_vw group by order_status")

after launching spark shell in pyspark , do all the above DF's and sqlcontext again 
orders_by_status.write.format("com.databricks.spark.avro").save("hdfs://localhost:54310/user/retail_db/orders_by_status_avro")

error: org.apache.avro.SchemaParseException: Illegal character in: count(1)
Note above error will come when you dont give alias in the sql
so you should give count(*) as order_status_cnt 
	orders_by_status=spark.sql("select order_status,count(*) as order_status_cnt from orders_vw group by order_status")


Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -ls /user/retail_db/orders_by_status_avro | wc -l 
     202
no. of files are more due to sparkSql partitioners 
so ,
sqlContext.setConf("spark.sql.shuffle.partitions", 2) 
Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -rm -r  /user/retail_db/orders_by_status_avro 

orders_by_status.write.format("com.databricks.spark.avro").save("hdfs://localhost:54310/user/retail_db/orders_by_status_avro")

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -ls /user/retail_db/orders_by_status_avro | wc -l 
       4

you can't read this avro file , so create sqlcontext to read 
sqlContext.read.format("com.databricks.spark.avro").load("hdfs://localhost:54310/user/retail_db/orders_by_status_avro").show()

+---------------+----------------+
|   order_status|order_status_cnt|
+---------------+----------------+
|PENDING_PAYMENT|           15030|
|     PROCESSING|            8275|
| PAYMENT_REVIEW|             729|
|        PENDING|            7610|
|        ON_HOLD|            3798|
|SUSPECTED_FRAUD|            1558|
|         CLOSED|            7556|
|       COMPLETE|           22899|
|       CANCELED|            1428|
+---------------+----------------+

in scala 
sqlContext.read.avro("hdfs://localhost:54310/user/retail_db/orders_by_status_avro").show()


this is all about the file formats 

========================================
lets do the compression 
so first delete the avro files 

hadoop fs -rm -r  /user/retail_db/orders_by_status_avro

-this is available fr avro and parquet only 
sqlContext.setConf("spark.sql.avro.compression.codec", "snappy") 

orders_by_status.write.format("com.databricks.spark.avro").save("hdfs://localhost:54310/user/retail_db/orders_by_status_avro")

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -du /user/retail_db/orders_by_status_avro
0    /user/retail_db/orders_by_status_avro/_SUCCESS
308  /user/retail_db/orders_by_status_avro/part-r-00000-a49af444-834b-408b-a56f-ca24d1f3ce02.avro
256  /user/retail_db/orders_by_status_avro/part-r-00001-a49af444-834b-408b-a56f-ca24d1f3ce02.avro


sqlContext.read.format("com.databricks.spark.avro").load("hdfs://localhost:54310/user/retail_db/orders_by_status_avro").show()

=======by doing gzip compression and repeating the above steps the size is little reduced 
sqlContext.setConf("spark.sql.avro.compression.codec", "gzip") 

Priyabrats-MacBook-Air:bin pbishwal$ hadoop fs -du /user/retail_db/orders_by_status_avro
0    /user/retail_db/orders_by_status_avro/_SUCCESS
308  /user/retail_db/orders_by_status_avro/part-r-00000-d7bd87b5-3004-4098-81cc-9c079e2cf183.avro
248  /user/retail_db/orders_by_status_avro/part-r-00001-d7bd87b5-3004-4098-81cc-9c079e2cf183.avro


for parquet:
sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy") 

=====>
convert from one file format to other i.e e.g convert from parquet to avro 
step 1: 
read the parquet DF after you have written 

read2avro_DF=sqlContext.read.parquet("hdfs://localhost:54310/user/retail_db/orders_by_status_parquet") 

step 2: now write that above DF to avro format 
read2avro_DF.write.format("com.databricks.spark.avro").save("hdfs://localhost:54310/user/retail_db/orders_by_status_pq2avro")

step3: read the DF 
sqlContext.read.format("com.databricks.spark.avro").load("hdfs://localhost:54310/user/retail_db/orders_by_status_pq2avro").show()

+---------------+----------------+
|   order_status|order_status_cnt|
+---------------+----------------+
|PENDING_PAYMENT|           15030|
|     PROCESSING|            8275|
| PAYMENT_REVIEW|             729|
|        PENDING|            7610|
|        ON_HOLD|            3798|
|SUSPECTED_FRAUD|            1558|
|         CLOSED|            7556|
|       COMPLETE|           22899|
|       CANCELED|            1428|
+---------------+----------------+

