
chapter 1 : Introduction 
Spark does not require Hadoop; it simply has support for storage systems implementing the Hadoop APIs.
Spark’s shells allow you to interact with data that is dis‐ tributed on disk or in memory across many machines, 
and Spark takes care of auto‐ matically distributing this processing.

Spark core 
At a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster.
The driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.
eg the driver program was the Spark shell 
Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. 
In the shell, a SparkContext is automatically created for you as the variable called sc.
To run these operations, driver programs typically manage a number of nodes called executors.

Standalone Applications
Spark can be linked into standalone applications in either Java, Scala, or Python. The main difference from using it
 in the shell is that you need to initialize your own SparkContext. After that, the API is the same.

The process of linking to Spark varies by language. In Java and Scala, you give your application a Maven dependency on the spark-core artifact.
In Python, you simply write applications as Python scripts, but you must run them using the bin/spark-submit script included in Spark.

Initializing a SparkContext

Once you have linked an application to Spark, you need to import the Spark packages in your program and create a SparkContext. 
You do so by first creating a SparkConf object to configure your application, and then building a SparkContext for it
from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)

These examples show the minimal way to initialize a SparkContext, where you pass two parameters:
• A cluster URL, namely local in these examples, which tells Spark how to connect to a cluster. 
	local is a special value that runs Spark on one thread on the local machine, without connecting to a cluster.
• An application name, namely My App in these examples. 
		This will identify your application on the cluster manager’s UI if you connect to a cluster.

Finally, to shut down Spark, you can either call the stop() method on your Spark‐ Context,
 or simply exit the application (e.g., with System.exit(0) or sys.exit()).
 
 --------------------------
chapter 2 : RDDs

 In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. 
 Under the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallalizes the operations you perform on them. 

RDD Basics:
An RDD in Spark is simply an immutable distributed collection of objects. 
Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. 
 
Users create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects (e.g., a list or set)
 in their driver program.  
 
RDDs offer two types of operations: transformations and actions.
Transformations construct a new RDD from a previous one. 
For example, one common transformation is filtering data that matches a predicate.
lines = sc.textFile("README.md")
pythonLines = lines.filter(lambda line: "Python" in line)

Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or 
save it to an external storage system (e.g., HDFS). 
Note : filter doesn't change the existing RDD rather it just applies a check on the lines RDD .
e.g 
>>> pythonLines.first()
u'## Interactive Python Shell' 


Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, 
Spark computes them only in a lazy fashion—that is, Transformed RDDs are computed lazily, only when you use them in an action.
Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is not immediately performed. 
Instead, Spark internally records metadata to indicate that this operation has been requested

Finally, Spark’s RDDs are by default recomputed each time you run an action on them. 
If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist(). 

After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), 
and reuse them in future actions. 
>>> pythonLines.persist
>>> pythonLines.count()
2
>>> pythonLines.first()
u'## Interactive Python Shell'

Creating RDDs
Spark provides two ways to create RDDs: loading an external dataset and parallelizing a collection in your driver program.

The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize() method,
lines = sc.parallelize(["pandas", "i like pandas"])
Loading from external storage 
e.g lines = sc.textFile("/path/to/README.md")

Note : Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.
e.g inputRDD -> apply filter (for errors) -> errorsRDD
    inputRDD -> apply filter (for warnings) -> warningRDD 
    union both -> errorsRDD ,warningRDD -> badLinesRDD 
 i.e  errorsRDD = inputRDD.filter(lambda x: "error" in x) 
 	  warningsRDD = inputRDD.filter(lambda x: "warning" in x) 
 	  badLinesRDD = errorsRDD.union(warningsRDD)   
    
map : 1 input to 1 output 
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x).collect()
flatmap : 1 input to multiple output ,Instead of returning a single element, we return an iterator with our return values. 
e.g 
lines = sc.parallelize(["hello world", "hi"]) 
words = lines.flatMap(lambda line: line.split(" ")) 
words.first() # returns "hello"

While intersection() and union() are two sim‐ ilar concepts, the performance of intersection() is much worse 
since it requires a shuffle over the network to identify common elements.


Actions
-------
 reduce(), which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type.
 sum = rdd.reduce(lambda x, y: x + y)
 
 fold(), which also takes a function with the same signature as needed for reduce(), but in addition takes a “zero value” to be used for the initial call on each partition. 
 The zero value you provide should be the identity element for your operation;
 
 collect(), which returns the entire RDD’s contents. collect() is commonly used in unit tests where the entire contents of the RDD
  are expected to fit in memory, as that makes it easy to compare the value of our RDD with our expected result. 
 
take(n) returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection.  

If there is an ordering defined on our data, we can also extract the top elements from an RDD using top().
top() will use the default ordering on the data, but we can supply our own comparison function to extract the top elements.

 --------------------------
chapter 3 : Working with Key Value paris :

Spark provides special operations on RDDs containing key/value pairs. These RDDs are called pair RDDs.

Creating Pair RDDs:
In Python, for the functions on keyed data to work we need to return an RDD composed of tuples 
Example  Creating a pair RDD using the first word as the key in Python
pairs=lines.map(lambda x:(x.split("")[0],x)) 
When creating a pair RDD from an in-memory collection in Scala and Python, we only need to call SparkContext.parallelize() on a collection of pairs.

Transformations on Pair RDDs
Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on indi‐ vidual elements.
rdd=( {(1, 2), (3, 4), (3, 6)})
rdd.reduceByKey((x,y)=>x+y)  o/p : {(1, 2), (3, 10)} 
rdd.groupByKey() o/p :  {(1, [2]), (3, [4, 6])} 
rdd.mapValues(x => x+1) : Apply a function to each value of a pair RDD without changing the key. {(1,3), (3,5) (3,7)}
- mapValues : means map the values , input is x i.e Key and transfrom the values corresponding to the keys by adding 1

flatMapValues(func) : Apply a function that returns an iterator to each value of a pair RDD, and for each element returned, 
   produce a key/value entry with the old key. Often used for tokenization.
for e.g rdd.flatMapValues(x => (x to 5) :  {(1,2),(1,3),(1,4),(1,5), (3,4) (3,5)} 

text=sc.textFile("Audio Standardization Sentences.txt")
text.collect()
[u'Oak is strong and also gives shade.', 
  u'Cats and dogs each hate the other.', 
  u'The pipe began to rust while new.',...]
  
# now i want to see total number of words in the text file 
words=text.flatMap(lambda x:x.split(" ")) # splits words on spaces 
words.collect()
--[u'Oak', u'is', u'strong'....]  
# the whole sentences in the text file is a single list in flatMap 
>>> words.count()
73

if you use map instead of flat map then it will produce each sentence with  a list of words 
e.g 
text.map(lambda x:x.split(" ")).collect() # splits words on spaces 
[ [u'Oak', u'is', u'strong', u'and', u'also', u'gives', u'shade.'], # this sentence is a list of words  
 [u'Cats', u'and', u'dogs', u'each', u'hate', u'the', u'other.'],
 ....]
 
 # map output is a list of lists 
 ----Map Partitions in RDD ----
 Map transfromation works on elements of the RDD 
 Map Partitions transfromation works on the partitions of the RDD . 
 Parition is one chunk of data in RDD 

text=sc.textFile("Audio Standardization Sentences.txt")
words=text.flatMap(lambda x:x.split(" ")) # splits words on spaces 


keys() :Return an RDD of just the keys. rdd.keys() {1, 3}
values :Return an RDD of just the values. rdd.values() {2,4,6}
sortByKey() : Return an RDD sorted by the key. rdd.sortByKey() {(1,2), (3,4) ,(3,6)}

(rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)})
cogroup : Group data from both RDDs sharing the same key. rdd.cogroup(other) 
rdd.cogroup(other) : {(1,([2],[])), (3, ([4, 6],[9]))} 

Aggregations:
-----
When datasets are described in terms of key/value pairs, it is common to want to aggregate statistics across all elements with the same key.
Note: These operations return RDDs and thus are transformations rather than actions. 

reduceByKey() runs several parallel reduce operations, one for each key in the dataset, 
 where each operation combines values that have the same key. 
e.g 
	Per-key average with reduceByKey() and mapValues() in Python
rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
# lambda x, y: in reduceByKey represents the o/p of map values i.e (0,1) (3,1) ....

key 	| value					 Key	| value					 key		| value
panda   	0					panda     (0,1)	x				 panda     (1,2)
pink	    3		Map Values 	pink	  (3,1)	y  reduceByKey   pink  	   (7,2)
pirate		3		=======>	pirate    (3,1)    =======>		 pirate    (3,1) 
panda 		1					panda     (1,1)
pink		4 					pink      (4,1)
  
  
  
Word count in Python
rdd = sc.textFile("s3://...")
words = rdd.flatMap(lambda x: x.split(" "))
result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
reduce by key works on adding values corresponding to matched keys . (this,1) (this,1) ==>reduceby key ==> (this,2)   
 x and y corresponds to the value part . 
 
 note: 
 words.map transforms as (this,1) (life,1) (this,1).. and reduceByKey adds the value with matching key  i.e (this,2) (life,1) ...
 instead of map you can also use mapValues 
 
Spark shared Variables:
------------------
When functions are passed to a specific Spark operation, it is executed on a particular remote cluster node.  
Usually, the operation is done in a way that different copy of variable(s) are used within the function. 
These particular variables are carefully copied into the different machines, and the updates to the variables in the said 
remote machines are not propagated back to the driver program. For this reason, one cannot support the general; 
read-write shared variables across the tasks and expects them to be efficient. Nevertheless, Spark does provide two different 
types (limited) of shared variables to two known usage patterns.

    Broadcast variables
    Accumulators
 
 Broadcast Variables :
 
	Broadcast variables allow Spark developers to keep a secured read-only variable cached on different nodes, 
	other than merely shipping a copy of it with the needed tasks. For an instance, they can be used to give a node a 
	copy of a large input dataset without having to waste time with network transfer I/O. 
	Spark has the ability to distribute broadcast variables using various broadcast algorithms which will in turn largely 
	reduce the cost of communication.
	   Actions in Spark can be executed through different stages. 
	   These stages are separated by distributed “shuffle” operations. Within each stage, Spark automatically broadcasts 
	   common data needed in a cached, serialized form which will be de-serialized by each node before the running of each task. 
	    For this reason, if you create broadcast variables explicitly, it should only be done when tasks across 
	    multiple stages are in need of same data. 
	
	>>> broadcastVar = sc.broadcast([1, 2, 3])
	pyspark.broadcast.Broadcast object at 0x102789f10>
	
	>> broadcastVar.value
	[1, 2, 3]
	
After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster 
so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast 
in order to ensure that all nodes get the same value of the broadcast variable 

Accumulators:
	Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore 
	be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. 
	Spark natively supports accumulators of numeric types, and programmers can add support for new types.
	>>> accum = sc.accumulator(0)
	>>> accum
	Accumulator<id=0, value=0>
	
	>>> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
	...
	10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s
	
	>>> accum.value
	10
	For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will 
	only be applied once, i.e. restarted tasks will not update the value. 
	In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.
	
	Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, 
	their value is only updated once that RDD is computed as part of an action. 
	Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). 
	
	The below code fragment demonstrates this property:
	accum = sc.accumulator(0)
	def g(x):
	    accum.add(x)
	    return f(x)
	data.map(g)
	# Here, accum is still 0 because no actions have caused the `map` to be computed.


 combineByKey() 
 ---------
 It is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it.
  combineByKey method requires three functions:
    createCombiner
    mergeValue
    mergeCombiner

val rdd = sc.parallelize(List(
  ("A", 3), ("A", 9), ("A", 12), ("A", 0), ("A", 5),("B", 4), 
  ("B", 10), ("B", 11), ("B", 20), ("B", 25),("C", 32), ("C", 91),
   ("C", 122), ("C", 3), ("C", 55)), 2)

rdd.combineByKey(
    (x:Int) => (x, 1),
    (acc:(Int, Int), x) => (acc._1 + x, acc._2 + 1),
    (acc1:(Int, Int), acc2:(Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2))


a. createCombiner creates the initial value (combiner) for a key's first encounter on a partition 
if one is not found --> (firstValueEncountered, 1). So, this is merely initializing a tuple with the first value and a key counter of 1.

b. Then, mergeValue is triggered only if a combiner (tuple in our case) has already been created for the found key on this 
   partition --> (existingTuple._1 + subSequentValue, existingTuple._2 + 1). 
   This adds the existing tuple's value (in the first slot) with the newly encountered value and takes the existing tuple's 
   counter (in the second slot) and increments it. So, we are   
   
c. mergeCombiner takes the combiners (tuples) created on each partition and merges them 
   together --> (tupleFromPartition._1 + tupleFromPartition2._1, tupleFromPartition1._2 + tupleFromPartition2._2). 
   This is merely adding the values from each tuple together and the counters together into one tuple.   
e,g   
let's break up a subset of your data into partitions and see it in action:
("A", 3), ("A", 9), ("B", 11) , ("A", 12),("B", 4), ("B", 10)

Partition 1
A=3 --> createCombiner(3) ==> accum[A] = (3, 1) --> 1st time encounter A  (x,1) represent (3,1) 
A=9 --> mergeValue(accum[A], 9) ==> accum[A] = (3 + 9, 1 + 1) --> 2nd time encounter A , (acc:(Int, Int), x) -> ((3,1),9)
B=11 --> createCombiner(11) ==> accum[B] = (11, 1)

Partition 2
A=12 --> createCombiner(12) ==> accum[A] = (12, 1)
B=4 --> createCombiner(4) ==> accum[B] = (4, 1)
B=10 --> mergeValue(accum[B], 10) ==> accum[B] = (4 + 10, 1 + 1) 

Merge partitions together
A ==> mergeCombiner((12, 2), (12, 1)) ==> (12 + 12, 2 + 1) , (acc1:(Int, Int), acc2:(Int, Int) -> (12, 2), (12, 1)
B ==> mergeCombiner((11, 1), (14, 2)) ==> (11 + 14, 1 + 2)

So, you should get back an array something like this:

Array((A, (24, 3)), (B, (25, 3)))
  
   