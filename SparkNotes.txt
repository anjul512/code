
chapter 1 : Introduction 
Spark does not require Hadoop; it simply has support for storage systems implementing the Hadoop APIs.
Spark’s shells allow you to interact with data that is dis‐ tributed on disk or in memory across many machines, 
and Spark takes care of auto‐ matically distributing this processing.

Spark core 
At a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster.
The driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.
eg the driver program was the Spark shell 
Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. 
In the shell, a SparkContext is automatically created for you as the variable called sc.
To run these operations, driver programs typically manage a number of nodes called executors.

Standalone Applications
Spark can be linked into standalone applications in either Java, Scala, or Python. The main difference from using it
 in the shell is that you need to initialize your own SparkContext. After that, the API is the same.

The process of linking to Spark varies by language. In Java and Scala, you give your application a Maven dependency on the spark-core artifact.
In Python, you simply write applications as Python scripts, but you must run them using the bin/spark-submit script included in Spark.

Initializing a SparkContext

Once you have linked an application to Spark, you need to import the Spark packages in your program and create a SparkContext. 
You do so by first creating a SparkConf object to configure your application, and then building a SparkContext for it
from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)

These examples show the minimal way to initialize a SparkContext, where you pass two parameters:
• A cluster URL, namely local in these examples, which tells Spark how to connect to a cluster. 
	local is a special value that runs Spark on one thread on the local machine, without connecting to a cluster.
• An application name, namely My App in these examples. 
		This will identify your application on the cluster manager’s UI if you connect to a cluster.

Finally, to shut down Spark, you can either call the stop() method on your Spark‐ Context,
 or simply exit the application (e.g., with System.exit(0) or sys.exit()).
 
 --------------------------
chapter 2 : RDDs

 In Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. 
 Under the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallalizes the operations you perform on them. 

RDD Basics:
An RDD in Spark is simply an immutable distributed collection of objects. 
Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. 
 
Users create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects (e.g., a list or set)
 in their driver program.  
 
RDDs offer two types of operations: transformations and actions.
Transformations construct a new RDD from a previous one. 
For example, one common transformation is filtering data that matches a predicate.
lines = sc.textFile("README.md")
pythonLines = lines.filter(lambda line: "Python" in line)

Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or 
save it to an external storage system (e.g., HDFS). 
Note : filter doesn't change the existing RDD rather it just applies a check on the lines RDD .
e.g 
>>> pythonLines.first()
u'## Interactive Python Shell' 


Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, 
Spark computes them only in a lazy fashion—that is, Transformed RDDs are computed lazily, only when you use them in an action.
Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is not immediately performed. 
Instead, Spark internally records metadata to indicate that this operation has been requested

Finally, Spark’s RDDs are by default recomputed each time you run an action on them. 
If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using RDD.persist(). 

After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), 
and reuse them in future actions. 
>>> pythonLines.persist
>>> pythonLines.count()
2
>>> pythonLines.first()
u'## Interactive Python Shell'

Creating RDDs
Spark provides two ways to create RDDs: loading an external dataset and parallelizing a collection in your driver program.

The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize() method,
lines = sc.parallelize(["pandas", "i like pandas"])
Loading from external storage 
e.g lines = sc.textFile("/path/to/README.md")

Note : Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.
e.g inputRDD -> apply filter (for errors) -> errorsRDD
    inputRDD -> apply filter (for warnings) -> warningRDD 
    union both -> errorsRDD ,warningRDD -> badLinesRDD 
 i.e  errorsRDD = inputRDD.filter(lambda x: "error" in x) 
 	  warningsRDD = inputRDD.filter(lambda x: "warning" in x) 
 	  badLinesRDD = errorsRDD.union(warningsRDD)   
    
map : 1 input to 1 output 
nums = sc.parallelize([1, 2, 3, 4])
squared = nums.map(lambda x: x * x).collect()
flatmap : 1 input to multiple output ,Instead of returning a single element, we return an iterator with our return values. 
e.g 
lines = sc.parallelize(["hello world", "hi"]) 
words = lines.flatMap(lambda line: line.split(" ")) 
words.first() # returns "hello"

While intersection() and union() are two sim‐ ilar concepts, the performance of intersection() is much worse 
since it requires a shuffle over the network to identify common elements.


Actions
-------
 reduce(), which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type.
 sum = rdd.reduce(lambda x, y: x + y)
 
 fold(), which also takes a function with the same signature as needed for reduce(), but in addition takes a “zero value” to be used for the initial call on each partition. 
 The zero value you provide should be the identity element for your operation;
 
 collect(), which returns the entire RDD’s contents. collect() is commonly used in unit tests where the entire contents of the RDD
  are expected to fit in memory, as that makes it easy to compare the value of our RDD with our expected result. 
 
take(n) returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection.  

If there is an ordering defined on our data, we can also extract the top elements from an RDD using top().
top() will use the default ordering on the data, but we can supply our own comparison function to extract the top elements.

 --------------------------
chapter 3 : Working with Key Value paris :

Spark provides special operations on RDDs containing key/value pairs. These RDDs are called pair RDDs.

Creating Pair RDDs:
In Python, for the functions on keyed data to work we need to return an RDD composed of tuples 
Example  Creating a pair RDD using the first word as the key in Python
pairs=lines.map(lambda x:(x.split("")[0],x)) 
When creating a pair RDD from an in-memory collection in Scala and Python, we only need to call SparkContext.parallelize() on a collection of pairs.

Transformations on Pair RDDs
Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on indi‐ vidual elements.
rdd=( {(1, 2), (3, 4), (3, 6)})
rdd.reduceByKey((x,y)=>x+y)  o/p : {(1, 2), (3, 10)} 
rdd.groupByKey() o/p :  {(1, [2]), (3, [4, 6])} 
rdd.mapValues(x => x+1) : Apply a function to each value of a pair RDD without changing the key. {(1,3), (3,5) (3,7)}
- mapValues : means map the values , input is x i.e Key and transfrom the values corresponding to the keys by adding 1

flatMapValues(func) : Apply a function that returns an iterator to each value of a pair RDD, and for each element returned, 
   produce a key/value entry with the old key. Often used for tokenization.
for e.g rdd.flatMapValues(x => (x to 5) :  {(1,2),(1,3),(1,4),(1,5), (3,4) (3,5)} 

text=sc.textFile("Audio Standardization Sentences.txt")
text.collect()
[u'Oak is strong and also gives shade.', 
  u'Cats and dogs each hate the other.', 
  u'The pipe began to rust while new.',...]
  
# now i want to see total number of words in the text file 
words=text.flatMap(lambda x:x.split(" ")) # splits words on spaces 
words.collect()
--[u'Oak', u'is', u'strong'....]  
# the whole sentences in the text file is a single list in flatMap 
>>> words.count()
73

if you use map instead of flat map then it will produce each sentence with  a list of words 
e.g 
text.map(lambda x:x.split(" ")).collect() # splits words on spaces 
[ [u'Oak', u'is', u'strong', u'and', u'also', u'gives', u'shade.'], # this sentence is a list of words  
 [u'Cats', u'and', u'dogs', u'each', u'hate', u'the', u'other.'],
 ....]
 
 # map output is a list of lists 
 ----Map Partitions in RDD ----
 Map transfromation works on elements of the RDD 
 Map Partitions transfromation works on the partitions of the RDD . 
 Parition is one chunk of data in RDD 

text=sc.textFile("Audio Standardization Sentences.txt")
words=text.flatMap(lambda x:x.split(" ")) # splits words on spaces 


keys() :Return an RDD of just the keys. rdd.keys() {1, 3}
values :Return an RDD of just the values. rdd.values() {2,4,6}
sortByKey() : Return an RDD sorted by the key. rdd.sortByKey() {(1,2), (3,4) ,(3,6)}

(rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)})
cogroup : Group data from both RDDs sharing the same key. rdd.cogroup(other) 
rdd.cogroup(other) : {(1,([2],[])), (3, ([4, 6],[9]))} 

Aggregations:
-----
When datasets are described in terms of key/value pairs, it is common to want to aggregate statistics across all elements with the same key.
Note: These operations return RDDs and thus are transformations rather than actions. 

reduceByKey() runs several parallel reduce operations, one for each key in the dataset, 
 where each operation combines values that have the same key. 
e.g 
	Per-key average with reduceByKey() and mapValues() in Python
rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
# lambda x, y: in reduceByKey represents the o/p of map values i.e (0,1) (3,1) ....
# for reduceByKey x,y represents the values corresponds to the key becoz just before using it you must have got the key value pairs

key 	| value					 Key	| value					 key		| value
panda   	0					panda     (0,1)	x				 panda     (1,2)
pink	    3		Map Values 	pink	  (3,1)	y  reduceByKey   pink  	   (7,2)
pirate		3		=======>	pirate    (3,1)    =======>		 pirate    (3,1) 
panda 		1					panda     (1,1)
pink		4 					pink      (4,1)
  
  
  
Word count in Python
rdd = sc.textFile("s3://...")
words = rdd.flatMap(lambda x: x.split(" "))
result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
reduce by key works on adding values corresponding to matched keys . (this,1) (this,1) ==>reduceby key ==> (this,2)   
 x and y corresponds to the value part . 
 
 note: 
 words.map transforms as (this,1) (life,1) (this,1).. and reduceByKey adds the value with matching key  i.e (this,2) (life,1) ...
 instead of map you can also use mapValues 
 
Spark shared Variables:
------------------
When functions are passed to a specific Spark operation, it is executed on a particular remote cluster node.  
Usually, the operation is done in a way that different copy of variable(s) are used within the function. 
These particular variables are carefully copied into the different machines, and the updates to the variables in the said 
remote machines are not propagated back to the driver program. For this reason, one cannot support the general; 
read-write shared variables across the tasks and expects them to be efficient. Nevertheless, Spark does provide two different 
types (limited) of shared variables to two known usage patterns.

    Broadcast variables
    Accumulators
 
 Broadcast Variables :
 
	Broadcast variables allow Spark developers to keep a secured read-only variable cached on different nodes, 
	other than merely shipping a copy of it with the needed tasks. For an instance, they can be used to give a node a 
	copy of a large input dataset without having to waste time with network transfer I/O. 
	Spark has the ability to distribute broadcast variables using various broadcast algorithms which will in turn largely 
	reduce the cost of communication.
	   Actions in Spark can be executed through different stages. 
	   These stages are separated by distributed “shuffle” operations. Within each stage, Spark automatically broadcasts 
	   common data needed in a cached, serialized form which will be de-serialized by each node before the running of each task. 
	    For this reason, if you create broadcast variables explicitly, it should only be done when tasks across 
	    multiple stages are in need of same data. 
	
	>>> broadcastVar = sc.broadcast([1, 2, 3])
	pyspark.broadcast.Broadcast object at 0x102789f10>
	
	>> broadcastVar.value
	[1, 2, 3]
	
After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster 
so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast 
in order to ensure that all nodes get the same value of the broadcast variable 

Accumulators:
	Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore 
	be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. 
	Spark natively supports accumulators of numeric types, and programmers can add support for new types.
	>>> accum = sc.accumulator(0)
	>>> accum
	Accumulator<id=0, value=0>
	
	>>> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
	...
	10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s
	
	>>> accum.value
	10
	For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will 
	only be applied once, i.e. restarted tasks will not update the value. 
	In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.
	
	Accumulators do not change the lazy evaluation model of Spark. If they are being updated within an operation on an RDD, 
	their value is only updated once that RDD is computed as part of an action. 
	Consequently, accumulator updates are not guaranteed to be executed when made within a lazy transformation like map(). 
	
	The below code fragment demonstrates this property:
	accum = sc.accumulator(0)
	def g(x):
	    accum.add(x)
	    return f(x)
	data.map(g)
	# Here, accum is still 0 because no actions have caused the `map` to be computed.


 combineByKey() 
 ---------
 It is the most general of the per-key aggregation functions. Most of the other per-key combiners are implemented using it.
  combineByKey method requires three functions:
    createCombiner
    mergeValue
    mergeCombiner

val rdd = sc.parallelize(List(
  ("A", 3), ("A", 9), ("A", 12), ("A", 0), ("A", 5),("B", 4), 
  ("B", 10), ("B", 11), ("B", 20), ("B", 25),("C", 32), ("C", 91),
   ("C", 122), ("C", 3), ("C", 55)), 2)

rdd.combineByKey(
    (x:Int) => (x, 1),
    (acc:(Int, Int), x) => (acc._1 + x, acc._2 + 1),
    (acc1:(Int, Int), acc2:(Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2))


a. createCombiner creates the initial value (combiner) for a key's first encounter on a partition 
if one is not found --> (firstValueEncountered, 1). So, this is merely initializing a tuple with the first value and a key counter of 1.

b. Then, mergeValue is triggered only if a combiner (tuple in our case) has already been created for the found key on this 
   partition --> (existingTuple._1 + subSequentValue, existingTuple._2 + 1). 
   This adds the existing tuple's value (in the first slot) with the newly encountered value and takes the existing tuple's 
   counter (in the second slot) and increments it. So, we are   
   
c. mergeCombiner takes the combiners (tuples) created on each partition and merges them 
   together --> (tupleFromPartition._1 + tupleFromPartition2._1, tupleFromPartition1._2 + tupleFromPartition2._2). 
   This is merely adding the values from each tuple together and the counters together into one tuple.   
e,g   
let's break up a subset of your data into partitions and see it in action:
("A", 3), ("A", 9), ("B", 11) , ("A", 12),("B", 4), ("B", 10)

Partition 1
A=3 --> createCombiner(3) ==> accum[A] = (3, 1) --> 1st time encounter A  (x,1) represent (3,1) 
A=9 --> mergeValue(accum[A], 9) ==> accum[A] = (3 + 9, 1 + 1) --> 2nd time encounter A , (acc:(Int, Int), x) -> ((3,1),9)
B=11 --> createCombiner(11) ==> accum[B] = (11, 1)

Partition 2
A=12 --> createCombiner(12) ==> accum[A] = (12, 1)
B=4 --> createCombiner(4) ==> accum[B] = (4, 1)
B=10 --> mergeValue(accum[B], 10) ==> accum[B] = (4 + 10, 1 + 1) 

Merge partitions together
A ==> mergeCombiner((12, 2), (12, 1)) ==> (12 + 12, 2 + 1) , (acc1:(Int, Int), acc2:(Int, Int) -> (12, 2), (12, 1)
B ==> mergeCombiner((11, 1), (14, 2)) ==> (11 + 14, 1 + 2)

So, you should get back an array something like this:

Array((A, (24, 3)), (B, (25, 3)))
  
   
   
Tuning the level of parallelism
------------------------
   
How Spark decides how to split up the work ???
Every RDD has a fixed number of partitions that determine the degree of parallelism to use when executing operations on the RDD.
    
When performing aggregations or grouping operations, we can ask Spark to use a specific number of partitions. 
Spark will always try to infer a sensible default value based on the size of your cluster, but in some cases you will 
want to tune the level of parallelism for better performance.   

data = [("a", 3), ("b", 4), ("a", 1)]
sc.parallelize(data).reduceByKey(lambda x, y: x + y) # Default parallelism 
sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # Custom parallelism ,10 - no. of partitions 

repartition() : to change the partitioning of an RDD outside the context of grouping and aggregation operations 
It shuffles the data across the network to create a new set of partitions.  

coalesce() : optimized version of repartition()  
It allows avoiding data movement, but only if you are decreasing the number of RDD partitions.

To know whether you can safely call coalesce(), you can check the size of the RDD using rdd.partitions.size() in Java/Scala 
and rdd.getNumPartitions() in Python and make sure that you are coalescing it to fewer partitions than it currently has.
   
   
Grouping Data
------------
If our data is already keyed in the way we want, groupByKey() will group our data using the key in our RDD.
On an RDD consisting of keys of type K and values of type V, we get back an RDD of type [K, Iterable[V]].
   
rdd=( {(1, 2), (3, 4), (3, 6)})
rdd.groupByKey() o/p :  {(1, [2]), (3, [4, 6])} 

groupBy() works on unpaired data or data where we want to use a different condition besides equality on the current key. 
It takes a function that it applies to every element in the source RDD and uses the result to determine the key.

rdd.reduceByKey(func) produces the same RDD as rdd.groupByKey().mapValues(value => value.reduce(func)) 
but is more efficient as it avoids the step of creating a list of values for each key.

we can group data sharing the same key from multiple RDDs using a function called cogroup(). 
cogroup() over two RDDs sharing the same key type, K, with the respective value types V and W gives us 
back RDD[(K, (Iterable[V], Iterable[W]))].

(rdd = {(1, 2), (3, 4), (3, 6)} other = {(3, 9)})
cogroup : Group data from both RDDs sharing the same key. rdd.cogroup(other) 
rdd.cogroup(other) : {(1,([2],[])), (3, ([4, 6],[9]))} 

aggregateByKey
 ------
	aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 	
	When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated 
	using the given combine functions and a neutral "zero" value.
	 Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. 
	Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. 
	
	note : https://devopsrecipe.wordpress.com/2016/10/17/pyspark-aggregatebykey/  explains better way 
	
	reduceByKey and aggregateByKey are much more efficient than groupByKey and should be used for aggregations as much as possible.
	
	# Create rdd that is a list of characters
	sc.parallelize(list("aaaaabbbbcccdd")).map(lambda letter: (letter, {"value": letter})) \
	.aggregateByKey( # Value to start aggregation (passed as s to `lambda s, d`) 
					"start", 
					# Function to join final data type (string) and rdd data type
					lambda s, d: "[ %s %s ]" % (s, d["value"]),
					# Function to join two final data types.
					lambda s1, s2: "{ %s %s }" % (s1, s2), ).collect()
	
	Out[2]:
	[('a', '{ { [ start a ] [ [ start a ] a ] } [ [ start a ] a ] }'),
	 ('c', '{ [ start c ] [ [ start c ] c ] }'),
	 ('b', '{ { [ [ start b ] b ] [ start b ] } [ start b ] }'),
	 ('d', '[ [ start d ] d ]')]
	 
	i.e 
	a. Initial State (0,0) 
	b. for each row of data  (lambda currentState,newData : (currentState[0]+newData ,currentState[1]+1 ))
	c. for each reducer lambda rddA,rddB : (rddA[0]+rddB[0], rddA[1]+rddB[1] ) 
 
 example -2 
 ==========
 
 >> data = sc.parallelize([(“a”,1),(“b”,2),(“a”,2),(“b”,3)])
 >> data.getNumPartitions()
    4
 So here our each of the tuple in “data” rdd is in a separate partition.   
 
 >> data.glom().collect()   # shows data in every partition 
[[(‘a’, 1)], [(‘b’, 2)], [(‘a’, 2)], [(‘b’, 3)]]
 
 Our problem statement is to find out the sum of values for each key. But the “zeroValue” for accumulator is 1.
 “zeroValue” means the initial value that you have set .
 
 >> data.aggregateByKey(1,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()
[(‘a’, 5), (‘b’, 7)]

So what’s happening here?

This transformation has 3 parameters here:

    First one is the “zeroValue” which is the initial value of the accumulator, acc i.e 1 .
    Second parameter is a  sequencing function. Let’s term it as function-1.
    Third parameter is a combiner function. Let’s term it as function-2.

The “zeroValue” of aggregateByKey transformation is applied to accumulator on each partition.
When tuple (“a”,1) is passed to the aggregateByKey transformation, function-1 is applied as below:
 lambda acc,val: acc + val i.e lambda 1,1: 1 + 1 => 2 
 So the value of accumulator for partition-1 is now 2.
 
 Now when the second tuple having key “a”, which is (“a”,2) is passed to the transformation, 
 the value 2 is not directly added to the previously calculated value of 2.
 Since tuple (“a”,2) is in a separate partition, the accumulator is set to the initial “zeroValue” which is 1.
 Here function-1 plays as follows:
 lambda acc,val: acc + val i.e lambda 1,2: 1 + 2 => 3 
 
 Now, these values are passed on to the second function, which is the combiner function and are finally 
 added to give the result (2+3)=5 for key “a”.
 
 VVIMP: 
  example 3:
  =========
   let’s take another scenario where we have rdd with only a single partition instead of 4. 
   >> data = sc.parallelize([(“a”,1),(“b”,2),(“a”,2),(“b”,3)],1)  # here 1 denotes the partition number so only single partition created 
   
   >> data.getNumPartitions()
   1
   >> data.glom().collect()
   [[(‘a’, 1), (‘b’, 2), (‘a’, 2), (‘b’, 3)]]
   
   >> data.aggregateByKey(1,lambda acc,val: acc+val, lambda acc1, acc2: acc1+acc2).collect()
	[(‘a’, 4), (‘b’, 6)]
 
   note imp: In this case, since the partition is single, the accumulator value will not be initialized to “zeroValue” 
   			  when the 2nd tuple of key “a” is encountered.
   For the first tuple of key “a” i.e. (“a”,1), function-1 is applied as :
	lambda acc,val: acc + val i.e lambda 1,1: 1 + 1 => 2
   For the 2nd tuple of key “a” i.e.(“a”,2), function-1 is applied as:
	lambda acc,val: acc + val i.e lambda 2,2: 2 + 2 => 4 

	Since there is only one partition, there will be only one accumulator per key.
	So acc2 = 0 for both key “a” as well as “b”.	
	
You can replace groupByKey with reduceByKey to improve performance in some cases. 
reduceByKey performs map side combine which can reduce Network IO and shuffle size where as groupByKey will not perform map side combine. 

combineByKey is more general then aggregateByKey. Actually, the implementation of aggregateByKey, reduceByKey and groupByKey
is achieved by combineByKey. aggregateByKey is similar to reduceByKey but you can provide initial values when performing aggregation. 

As the name suggests, aggregateByKey is suitable for compute aggregations for keys, 
e.g  aggregations such as sum, avg, etc. The rule here is that the extra computation spent for map side combine can reduce 
the size sent out to other nodes and driver. If your func has satisfies this rule, you probably should use aggregateByKey.  

combineByKey is more general and you have the flexibility to specify whether you'd like to perform map side combine. 
However, it is more complex to use. At minimum, you need to implement three functions: createCombiner, mergeValue, mergeCombiners. 


Sorting Data	
-----------
We can sort an RDD with key/value pairs provided that there is an ordering defined on the key.  
Once we have sorted our data, any subsequent call on the sorted data to collect() or save() will result in ordered data			  
sortByKey() function takes a parameter called ascending indicating whether we want it in ascending order (it defaults to true).

rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))  
>>> rdd2 =  sc.parallelize([("foo", 4), ("bar", 5), ("bar", 6)])
>>> rdd2.sortByKey(ascending=True,numPartitions=None, keyfunc = lambda x: str(x)) .collect()
[('bar', 5), ('bar', 6), ('foo', 4)]
 
Joins
-----
>>> rdd1 =  sc.parallelize([("foo", 1), ("bar", 2), ("baz", 3)])
>>> rdd2 =  sc.parallelize([("foo", 4), ("bar", 5), ("bar", 6)])
>>> rdd1.getNumPartitions()
4
>>> rdd2.getNumPartitions()
4
>>> rdd1.join(rdd2).collect()
[('foo', (1, 4)), ('bar', (2, 5)), ('bar', (2, 6))]                             
>>> rdd1.leftOuterJoin(rdd2).collect()
[('foo', (1, 4)), ('baz', (3, None)), ('bar', (2, 5)), ('bar', (2, 6))]
>>> rdd1.rightOuterJoin(rdd2).collect()
[('foo', (1, 4)), ('bar', (2, 5)), ('bar', (2, 6))]
>>> rdd1.cartesian(rdd2).collect()
[(('foo', 1), ('foo', 4)), (('foo', 1), ('bar', 5)), (('foo', 1), ('bar', 6)), (('bar', 2), ('foo', 4)), (('bar', 2), ('bar', 5)), (('bar', 2), ('bar', 6)), (('baz', 3), ('foo', 4)), (('baz', 3), ('bar', 5)), (('baz', 3), ('bar', 6))]



Actions Available on Pair RDDs
-----------------------
>>> rdd2 =  sc.parallelize([("foo", 4), ("bar", 5), ("bar", 6)])
>>> rdd2.countByKey()  # Count the number of elements for each  key.
defaultdict(<type 'int'>, {'foo': 1, 'bar': 2})

>>> rdd=sc.parallelize({(1, 2), (3, 4), (3, 6)})
>>> rdd.collectAsMap()  # Collect the result as a map to provide easy lookup.
{1: 2, 3: 6}

>>> rdd.lookup(3) # Return all values associated with the provided key.
[4, 6]

Data Partitioning (Advanced)
------------------------
Spark programs can choose to control their RDDs’ partitioning to reduce communication .
It is useful only when a dataset is reused multiple times in key-oriented operations such as joins.
Spark’s partitioning is available on all RDDs of key/value pairs, and causes the system to group elements based on a function of each key. 


Operations That Benefit from Partitioning
As of Spark 1.0, the operations that benefit from partitioning are cogroup(), groupWith(), join(), leftOuterJoin(), rightOuter Join(), groupByKey(), reduceByKey(), combineByKey(), and lookup().

if you call map() on a hash-partitioned RDD of key/value pairs, the function passed to map() can in theory change the key of each element, so the result will not have a partitioner. Spark does not analyze your functions to check whether they retain the key. Instead, it provides two other operations, mapValue



  
   
