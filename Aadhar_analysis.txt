CREATE TABLE iTest.uid_enrolments_detail(
  registrar string, 
  enrolment_agency string, 
  state string, 
  district string, 
  sub_district string, 
  pin_code bigint, 
  gender string, 
  age int, 
  aadhaar_generated int, 
  enrolment_rejected int, 
  residents_providing_email int, 
  residents_providing_mobile_number int)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY ',' 
LOCATION '/user/hive/iTest/uid_enrolments_detail'
TBLPROPERTIES ('skip.header.line.count'='1');

LOAD DATA  LOCAL INPATH '/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/Taming_BigData_WithSpark/aadhar.csv' INTO TABLE  iTest.uid_enrolments_detail;

Problem Statement:
CSV file contains following columns: Registrar, Enrollment Agency, State, District, Sub District, Pin Code, Gender, Age, Aadhaar Generated, Enrollment Rejected, Residents providing email, Residents providing mobile number
Count the number of Identities generated in each state
Count the number of Identities generated by each Enrollment Agency
Top 10 districts with maximum identities generated for both Male and Female


SELECT state, sum(aadhaar_generated) as cnt FROM iTest.uid_enrolments_detail GROUP BY state order by state ;
When we run the query to get number of Aadhaars generated for each state:
  First mapreduce job computes the total Aadhaars generated for each state. 
  Second mapreduce job sorts the States according to number of Aadhaars generated in descending order,

Query ID = pbishwal_20170424195915_98942cfb-3c16-4c09-8da3-e7d06095be28
Total jobs = 2
Launching Job 1 out of 2
.............
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):


Spark:
=======
We deviate from our usual approach of using Spark RDD transformations to do our job and take a look at Spark SQL to process the dataset using Dataframes.
Dataframe is an immutable collection of data distributed across nodes similar to RDD. 
Dataframes are similar to tables in RDBMS in that data is organized into named columns. 
Hive tables can be read as dataframes or any existing RDDs can be converted to dataframes by imposing a structure on it.

from pyspark.sql import HiveContext
from pyspark import SQLContext
hiveCtx = HiveContext(sc)
sqlContext = SQLContext(sc)
uidEnrolmentDF = sqlContext.read.load("/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/Taming_BigData_WithSpark/aadhar.csv",format='com.databricks.spark.csv',header='true',inferSchema='true')
uidEnrolmentDF.registerTempTable("uid_enrolments_detail")

Note : need to give chmod 777 /tmp/hive else it would give The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x

stateWiseCountDF = hiveCtx.sql("SELECT state, sum(aadhaar_generated) as cnt FROM iTest.uid_enrolments_detail GROUP BY state order by state ")
stateWiseCountDF.write.saveAsTable('iTest.state_wise_count',path="/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/Taming_BigData_WithSpark",mode="overwrite" )

stateWiseCountDF.saveAsTextFile("/Users/pbishwal/Documents/Techie/SparknScala/SparkCodes/Taming_BigData_WithSpark/") 


 
departmentsJson.registerTempTable("departmentsTable")

rows = hiveCtx.sql("select name,code from iTest.states_raw")
firstRow = rows.first()
print firstRow.name






